{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74bc63d5",
   "metadata": {},
   "source": [
    "# Data Cleaning \n",
    "\n",
    "Here we'll dig into cleaning the data, importing the cached data quality checks we generated automatically at the tail-end of our 01_data_quality-check.ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd98586",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46abdeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# Add paths for our custom modules\n",
    "sys.path.append('../../shared')\n",
    "sys.path.append('../../step2_data_ingestion')\n",
    "\n",
    "# Import our modules\n",
    "from sheets_client import open_sheet\n",
    "from config_manager import load_settings\n",
    "from schema import SchemaManager\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2adfa6b",
   "metadata": {},
   "source": [
    "## Fetching the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839df38c",
   "metadata": {},
   "source": [
    "### Load Original Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c38405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original datasets for cleaning...\n",
      "âœ… Loaded analysis results from ../data/processed/licenses_df.pkl\n",
      "   LOADED business_licenses: 2040 rows from cache\n",
      "âœ… Loaded analysis results from ../data/processed/permits_df.pkl\n",
      "   LOADED building_permits: 8647 rows from cache\n",
      "âœ… Loaded analysis results from ../data/processed/cta_df.pkl\n",
      "   LOADED cta_boardings: 668 rows from cache\n",
      "\n",
      "DATASETS READY FOR CLEANING:\n",
      "   Business Licenses: 2,040 rows\n",
      "   Building Permits: 8,647 rows\n",
      "   CTA Boardings: 668 rows\n",
      "   Total records: 11,355\n",
      "\n",
      "Original datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load original datasets (same loading logic as quality check notebook)\n",
    "sys.path.append('../../shared')\n",
    "from notebook_utils import *\n",
    "\n",
    "# Define dataset configurations\n",
    "datasets_config = {\n",
    "    'business_licenses': {\n",
    "        'worksheet': 'Business_Licenses_Full',\n",
    "        'pickle_name': 'licenses_df'\n",
    "    },\n",
    "    'building_permits': {\n",
    "        'worksheet': 'Building_Permits_Full',\n",
    "        'pickle_name': 'permits_df'\n",
    "    },\n",
    "    'cta_boardings': {\n",
    "        'worksheet': 'CTA_Full',\n",
    "        'pickle_name': 'cta_df'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load datasets from cache first\n",
    "datasets = {}\n",
    "load_from_sheets = False\n",
    "\n",
    "print(\"Loading original datasets for cleaning...\")\n",
    "for dataset_name, config in datasets_config.items():\n",
    "    try:\n",
    "        df = load_analysis_results(config['pickle_name'])\n",
    "        if df.empty:\n",
    "            raise FileNotFoundError(f\"{config['pickle_name']} is empty\")\n",
    "        datasets[dataset_name] = df\n",
    "        print(f\"   LOADED {dataset_name}: {len(df)} rows from cache\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   CACHE MISS {dataset_name}: will load from sheets\")\n",
    "        load_from_sheets = True\n",
    "\n",
    "if load_from_sheets:\n",
    "    print(\"\\nLoading fresh data from Google Sheets...\")\n",
    "    settings = load_settings()\n",
    "    sh = open_sheet(settings.sheet_id, settings.google_creds_path)\n",
    "\n",
    "    for dataset_name, config in datasets_config.items():\n",
    "        if dataset_name not in datasets:\n",
    "            df = load_sheet_data(sh, config['worksheet'])\n",
    "            datasets[dataset_name] = df\n",
    "            save_analysis_results(df, config['pickle_name'])\n",
    "            print(f\"   LOADED {dataset_name}: {len(df)} rows from sheets and cached\")\n",
    "\n",
    "# Extract for easier access\n",
    "licenses_df = datasets['business_licenses'].copy()  # Use .copy() so we don't modify originals\n",
    "permits_df = datasets['building_permits'].copy()\n",
    "cta_df = datasets['cta_boardings'].copy()\n",
    "\n",
    "print(f\"\\nDATASETS READY FOR CLEANING:\")\n",
    "print(f\"   Business Licenses: {len(licenses_df):,} rows\")\n",
    "print(f\"   Building Permits: {len(permits_df):,} rows\")\n",
    "print(f\"   CTA Boardings: {len(cta_df):,} rows\")\n",
    "print(f\"   Total records: {len(licenses_df) + len(permits_df) + len(cta_df):,}\")\n",
    "\n",
    "print(f\"\\nOriginal datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f43c44",
   "metadata": {},
   "source": [
    "### Quality-Driven Cleaning Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf61c932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING QUALITY ANALYSIS RESULTS\n",
      "==================================================\n",
      "   ðŸ“‚ Loading latest batch: data_quality_batch_20250901_153632\n",
      "   âœ… quality: quality_matrix.pkl\n",
      "   âœ… contamination: contamination_analysis.pkl\n",
      "   âœ… anomalies: anomaly_detection.pkl\n",
      "   âœ… business_validation: business_validation.pkl\n",
      "   âœ… completeness: completeness_analysis.pkl\n",
      "   âœ… Summary: analysis_summary.json\n",
      "\n",
      "ðŸŽ¯ QUALITY ANALYSIS RESULTS READY!\n",
      "   ðŸ“Š Quality Score: 0.988\n",
      "   ðŸ” Contaminated Fields: 7\n",
      "   ðŸ”§ Recommendations: 2\n",
      "AVAILABLE QUALITY ANALYSIS BATCHES\n",
      "========================================\n",
      " 1. data_quality_batch_20250901_153632\n",
      "     ðŸ“… 2025-09-01T15:36:32\n",
      "     ðŸ“Š Quality: 0.988\n",
      "     ðŸ”§ Recommendations: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Quality Analysis Results Loader\n",
    "def load_quality_analysis_results(batch_id=None):\n",
    "    \"\"\"\n",
    "    Load quality analysis results from latest batch or specific batch.\n",
    "\n",
    "    Args:\n",
    "        batch_id (str, optional): Specific batch ID to load. If None, loads latest.\n",
    "    \"\"\"\n",
    "    print(\"LOADING QUALITY ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    results_dir = Path(\"../../data/quality_analysis\")\n",
    "\n",
    "    if not results_dir.exists():\n",
    "        print(\"âŒ No quality analysis results found!\")\n",
    "        print(\"   Please run 01_data_quality_check.ipynb first\")\n",
    "        return None\n",
    "\n",
    "    # Determine which batch to load\n",
    "    if batch_id:\n",
    "        batch_dir = results_dir / batch_id\n",
    "        if not batch_dir.exists():\n",
    "            print(f\"âŒ Batch '{batch_id}' not found!\")\n",
    "            return None\n",
    "        print(f\"   ðŸ“‚ Loading specific batch: {batch_id}\")\n",
    "    else:\n",
    "        # Load from latest directory\n",
    "        batch_dir = results_dir / \"latest\"\n",
    "        if not batch_dir.exists():\n",
    "            print(\"âŒ No latest results found!\")\n",
    "            return None\n",
    "\n",
    "        # Get batch info from latest\n",
    "        batch_info_file = batch_dir / \"latest_batch_info.txt\"\n",
    "        if batch_info_file.exists():\n",
    "            with open(batch_info_file, 'r') as f:\n",
    "                batch_info = f.read()\n",
    "                batch_id = [line for line in batch_info.split('\\n') if 'Batch ID:' in line][0].split(': ')[1]\n",
    "        print(f\"   ðŸ“‚ Loading latest batch: {batch_id}\")\n",
    "\n",
    "    # Load all analysis components\n",
    "    analysis_results = {'batch_id': batch_id}\n",
    "\n",
    "    # Load files with error handling\n",
    "    files_to_load = {\n",
    "        'quality': 'quality_matrix.pkl',\n",
    "        'contamination': 'contamination_analysis.pkl',\n",
    "        'anomalies': 'anomaly_detection.pkl',\n",
    "        'business_validation': 'business_validation.pkl',\n",
    "        'completeness': 'completeness_analysis.pkl'\n",
    "    }\n",
    "\n",
    "    for key, filename in files_to_load.items():\n",
    "        file_path = batch_dir / filename\n",
    "        if file_path.exists():\n",
    "            with open(file_path, 'rb') as f:\n",
    "                analysis_results[key] = pickle.load(f)\n",
    "            print(f\"   âœ… {key}: {filename}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  {key}: {filename} (missing)\")\n",
    "\n",
    "    # Load summary if available\n",
    "    summary_file = batch_dir / \"analysis_summary.json\"\n",
    "    if summary_file.exists():\n",
    "        with open(summary_file, 'r') as f:\n",
    "            analysis_results['summary'] = json.load(f)\n",
    "        print(f\"   âœ… Summary: analysis_summary.json\")\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ QUALITY ANALYSIS RESULTS READY!\")\n",
    "    if 'summary' in analysis_results:\n",
    "        summary = analysis_results['summary']\n",
    "        print(f\"   ðŸ“Š Quality Score: {summary['quality_summary']['overall_score']:.3f}\")\n",
    "        print(f\"   ðŸ” Contaminated Fields: {summary['quality_summary']['contaminated_fields']}\")\n",
    "        print(f\"   ðŸ”§ Recommendations: {summary['completeness_summary']['total_recommendations']}\")\n",
    "\n",
    "    return analysis_results\n",
    "\n",
    "# Function to list available batches\n",
    "def list_available_batches():\n",
    "    \"\"\"List all available quality analysis batches.\"\"\"\n",
    "    results_dir = Path(\"../../data/quality_analysis\")\n",
    "    index_file = results_dir / \"batch_index.json\"\n",
    "\n",
    "    if not index_file.exists():\n",
    "        print(\"ðŸ“‚ No batch index found\")\n",
    "        return []\n",
    "\n",
    "    with open(index_file, 'r') as f:\n",
    "        batch_index = json.load(f)\n",
    "\n",
    "    print(\"AVAILABLE QUALITY ANALYSIS BATCHES\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    for i, batch in enumerate(batch_index['batches'][:10]):  # Show last 10\n",
    "        print(f\"{i+1:2d}. {batch['batch_id']}\")\n",
    "        print(f\"     ðŸ“… {batch['datetime'][:19]}\")\n",
    "        print(f\"     ðŸ“Š Quality: {batch['summary_scores']['overall_quality']:.3f}\")\n",
    "        print(f\"     ðŸ”§ Recommendations: {batch['summary_scores']['total_recommendations']}\")\n",
    "        print()\n",
    "\n",
    "    return batch_index['batches']\n",
    "\n",
    "# Load latest results by default\n",
    "quality_results = load_quality_analysis_results()\n",
    "\n",
    "# Optionally list available batches\n",
    "available_batches = list_available_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc17eeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CLEANING PRIORITY ANALYSIS\n",
      "==================================================\n",
      "SUMMARY FROM QUALITY ANALYSIS:\n",
      "   Overall Quality Score: 0.988\n",
      "   Contaminated Fields: 7\n",
      "   Critical Issues: 0\n",
      "\n",
      "CLEANING RECOMMENDATIONS:\n",
      "   Total Recommendations: 2\n",
      "   Critical Imputation Needed: 0\n",
      "   Low-value Fields: 2\n",
      "\n",
      "CONTAMINATION ISSUES BY DATASET:\n",
      "   business_licenses: 1 contamination issues\n",
      "   building_permits: 3 contamination issues\n",
      "\n",
      "READY TO BEGIN TARGETED DATA CLEANING!\n",
      "   High Priority Actions: 1\n",
      "   Critical Fixes: 0\n"
     ]
    }
   ],
   "source": [
    "# Extract key insights from quality analysis for cleaning priorities\n",
    "def analyze_cleaning_priorities(quality_results):\n",
    "    \"\"\"Extract actionable cleaning priorities from quality analysis.\"\"\"\n",
    "    print(\"DATA CLEANING PRIORITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if not quality_results:\n",
    "        print(\"ERROR: No quality results available\")\n",
    "        return None\n",
    "\n",
    "    summary = quality_results.get('summary', {})\n",
    "    contamination = quality_results.get('contamination', {})\n",
    "    completeness = quality_results.get('completeness', {})\n",
    "\n",
    "    # Extract high-priority cleaning actions\n",
    "    cleaning_plan = {\n",
    "        'critical_fixes': [],\n",
    "        'high_priority': [],\n",
    "        'medium_priority': [],\n",
    "        'field_specific_actions': {}\n",
    "    }\n",
    "\n",
    "    print(f\"SUMMARY FROM QUALITY ANALYSIS:\")\n",
    "    if 'quality_summary' in summary:\n",
    "        qs = summary['quality_summary']\n",
    "        print(f\"   Overall Quality Score: {qs.get('overall_score', 0):.3f}\")\n",
    "        print(f\"   Contaminated Fields: {qs.get('contaminated_fields', 0)}\")\n",
    "        print(f\"   Critical Issues: {qs.get('critical_issues', 0)}\")\n",
    "\n",
    "    print(f\"\\nCLEANING RECOMMENDATIONS:\")\n",
    "    if 'completeness_summary' in summary:\n",
    "        cs = summary['completeness_summary']\n",
    "        print(f\"   Total Recommendations: {cs.get('total_recommendations', 0)}\")\n",
    "        print(f\"   Critical Imputation Needed: {cs.get('critical_imputation_needed', 0)}\")\n",
    "        print(f\"   Low-value Fields: {cs.get('low_value_fields', 0)}\")\n",
    "\n",
    "    # Extract contamination issues for each dataset\n",
    "    print(f\"\\nCONTAMINATION ISSUES BY DATASET:\")\n",
    "    for dataset_name, contamination_info in contamination.items():\n",
    "        if isinstance(contamination_info, dict):\n",
    "            total_issues = (\n",
    "                len(contamination_info.get('mixed_types', [])) +\n",
    "                len(contamination_info.get('special_chars', [])) +\n",
    "                len(contamination_info.get('encoding_issues', [])) +\n",
    "                len(contamination_info.get('format_inconsistencies', []))\n",
    "            )\n",
    "            if total_issues > 0:\n",
    "                print(f\"   {dataset_name}: {total_issues} contamination issues\")\n",
    "\n",
    "                # Extract specific recommendations\n",
    "                recommendations = contamination_info.get('recommendations', [])\n",
    "                for rec in recommendations:\n",
    "                    cleaning_plan['high_priority'].append({\n",
    "                        'dataset': dataset_name,\n",
    "                        'action': rec,\n",
    "                        'type': 'contamination_fix'\n",
    "                    })\n",
    "\n",
    "    return cleaning_plan\n",
    "\n",
    "# Analyze cleaning priorities from our quality results\n",
    "cleaning_priorities = analyze_cleaning_priorities(quality_results)\n",
    "\n",
    "print(f\"\\nREADY TO BEGIN TARGETED DATA CLEANING!\")\n",
    "if cleaning_priorities:\n",
    "    print(f\"   High Priority Actions: {len(cleaning_priorities['high_priority'])}\")\n",
    "    print(f\"   Critical Fixes: {len(cleaning_priorities['critical_fixes'])}\")\n",
    "else:\n",
    "    print(\"   Will proceed with standard cleaning workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c108b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
