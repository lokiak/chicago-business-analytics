{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74bc63d5",
   "metadata": {},
   "source": [
    "# Data Cleaning \n",
    "\n",
    "Here we'll dig into cleaning the data, importing the cached data quality checks we generated automatically at the tail-end of our 01_data_quality-check.ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd98586",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46abdeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# Add paths for our custom modules\n",
    "sys.path.append('../../shared')\n",
    "sys.path.append('../../step2_data_ingestion')\n",
    "\n",
    "# Import our modules\n",
    "from sheets_client import open_sheet\n",
    "from config_manager import load_settings\n",
    "from schema import SchemaManager\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2adfa6b",
   "metadata": {},
   "source": [
    "## Fetching the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839df38c",
   "metadata": {},
   "source": [
    "### Load Original Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c38405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original datasets for cleaning...\n",
      "âœ… Loaded analysis results from ../data/processed/licenses_df.pkl\n",
      "   LOADED business_licenses: 2040 rows from cache\n",
      "âœ… Loaded analysis results from ../data/processed/permits_df.pkl\n",
      "   LOADED building_permits: 8647 rows from cache\n",
      "âœ… Loaded analysis results from ../data/processed/cta_df.pkl\n",
      "   LOADED cta_boardings: 668 rows from cache\n",
      "\n",
      "DATASETS READY FOR CLEANING:\n",
      "   Business Licenses: 2,040 rows\n",
      "   Building Permits: 8,647 rows\n",
      "   CTA Boardings: 668 rows\n",
      "   Total records: 11,355\n",
      "\n",
      "Original datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load original datasets (same loading logic as quality check notebook)\n",
    "sys.path.append('../../shared')\n",
    "from notebook_utils import *\n",
    "\n",
    "# Define dataset configurations\n",
    "datasets_config = {\n",
    "    'business_licenses': {\n",
    "        'worksheet': 'Business_Licenses_Full',\n",
    "        'pickle_name': 'licenses_df'\n",
    "    },\n",
    "    'building_permits': {\n",
    "        'worksheet': 'Building_Permits_Full',\n",
    "        'pickle_name': 'permits_df'\n",
    "    },\n",
    "    'cta_boardings': {\n",
    "        'worksheet': 'CTA_Full',\n",
    "        'pickle_name': 'cta_df'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load datasets from cache first\n",
    "datasets = {}\n",
    "load_from_sheets = False\n",
    "\n",
    "print(\"Loading original datasets for cleaning...\")\n",
    "for dataset_name, config in datasets_config.items():\n",
    "    try:\n",
    "        df = load_analysis_results(config['pickle_name'])\n",
    "        if df.empty:\n",
    "            raise FileNotFoundError(f\"{config['pickle_name']} is empty\")\n",
    "        datasets[dataset_name] = df\n",
    "        print(f\"   LOADED {dataset_name}: {len(df)} rows from cache\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   CACHE MISS {dataset_name}: will load from sheets\")\n",
    "        load_from_sheets = True\n",
    "\n",
    "if load_from_sheets:\n",
    "    print(\"\\nLoading fresh data from Google Sheets...\")\n",
    "    settings = load_settings()\n",
    "    sh = open_sheet(settings.sheet_id, settings.google_creds_path)\n",
    "\n",
    "    for dataset_name, config in datasets_config.items():\n",
    "        if dataset_name not in datasets:\n",
    "            df = load_sheet_data(sh, config['worksheet'])\n",
    "            datasets[dataset_name] = df\n",
    "            save_analysis_results(df, config['pickle_name'])\n",
    "            print(f\"   LOADED {dataset_name}: {len(df)} rows from sheets and cached\")\n",
    "\n",
    "# Extract for easier access\n",
    "licenses_df = datasets['business_licenses'].copy()  # Use .copy() so we don't modify originals\n",
    "permits_df = datasets['building_permits'].copy()\n",
    "cta_df = datasets['cta_boardings'].copy()\n",
    "\n",
    "print(f\"\\nDATASETS READY FOR CLEANING:\")\n",
    "print(f\"   Business Licenses: {len(licenses_df):,} rows\")\n",
    "print(f\"   Building Permits: {len(permits_df):,} rows\")\n",
    "print(f\"   CTA Boardings: {len(cta_df):,} rows\")\n",
    "print(f\"   Total records: {len(licenses_df) + len(permits_df) + len(cta_df):,}\")\n",
    "\n",
    "print(f\"\\nOriginal datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f43c44",
   "metadata": {},
   "source": [
    "### Quality-Driven Cleaning Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf61c932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING QUALITY ANALYSIS RESULTS\n",
      "==================================================\n",
      "   ðŸ“‚ Loading latest batch: data_quality_batch_20250901_153632\n",
      "   âœ… quality: quality_matrix.pkl\n",
      "   âœ… contamination: contamination_analysis.pkl\n",
      "   âœ… anomalies: anomaly_detection.pkl\n",
      "   âœ… business_validation: business_validation.pkl\n",
      "   âœ… completeness: completeness_analysis.pkl\n",
      "   âœ… Summary: analysis_summary.json\n",
      "\n",
      "ðŸŽ¯ QUALITY ANALYSIS RESULTS READY!\n",
      "   ðŸ“Š Quality Score: 0.988\n",
      "   ðŸ” Contaminated Fields: 7\n",
      "   ðŸ”§ Recommendations: 2\n",
      "AVAILABLE QUALITY ANALYSIS BATCHES\n",
      "========================================\n",
      " 1. data_quality_batch_20250901_153632\n",
      "     ðŸ“… 2025-09-01T15:36:32\n",
      "     ðŸ“Š Quality: 0.988\n",
      "     ðŸ”§ Recommendations: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Quality Analysis Results Loader\n",
    "def load_quality_analysis_results(batch_id=None):\n",
    "    \"\"\"\n",
    "    Load quality analysis results from latest batch or specific batch.\n",
    "\n",
    "    Args:\n",
    "        batch_id (str, optional): Specific batch ID to load. If None, loads latest.\n",
    "    \"\"\"\n",
    "    print(\"LOADING QUALITY ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    results_dir = Path(\"../../data/quality_analysis\")\n",
    "\n",
    "    if not results_dir.exists():\n",
    "        print(\"âŒ No quality analysis results found!\")\n",
    "        print(\"   Please run 01_data_quality_check.ipynb first\")\n",
    "        return None\n",
    "\n",
    "    # Determine which batch to load\n",
    "    if batch_id:\n",
    "        batch_dir = results_dir / batch_id\n",
    "        if not batch_dir.exists():\n",
    "            print(f\"âŒ Batch '{batch_id}' not found!\")\n",
    "            return None\n",
    "        print(f\"   ðŸ“‚ Loading specific batch: {batch_id}\")\n",
    "    else:\n",
    "        # Load from latest directory\n",
    "        batch_dir = results_dir / \"latest\"\n",
    "        if not batch_dir.exists():\n",
    "            print(\"âŒ No latest results found!\")\n",
    "            return None\n",
    "\n",
    "        # Get batch info from latest\n",
    "        batch_info_file = batch_dir / \"latest_batch_info.txt\"\n",
    "        if batch_info_file.exists():\n",
    "            with open(batch_info_file, 'r') as f:\n",
    "                batch_info = f.read()\n",
    "                batch_id = [line for line in batch_info.split('\\n') if 'Batch ID:' in line][0].split(': ')[1]\n",
    "        print(f\"   ðŸ“‚ Loading latest batch: {batch_id}\")\n",
    "\n",
    "    # Load all analysis components\n",
    "    analysis_results = {'batch_id': batch_id}\n",
    "\n",
    "    # Load files with error handling\n",
    "    files_to_load = {\n",
    "        'quality': 'quality_matrix.pkl',\n",
    "        'contamination': 'contamination_analysis.pkl',\n",
    "        'anomalies': 'anomaly_detection.pkl',\n",
    "        'business_validation': 'business_validation.pkl',\n",
    "        'completeness': 'completeness_analysis.pkl'\n",
    "    }\n",
    "\n",
    "    for key, filename in files_to_load.items():\n",
    "        file_path = batch_dir / filename\n",
    "        if file_path.exists():\n",
    "            with open(file_path, 'rb') as f:\n",
    "                analysis_results[key] = pickle.load(f)\n",
    "            print(f\"   âœ… {key}: {filename}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  {key}: {filename} (missing)\")\n",
    "\n",
    "    # Load summary if available\n",
    "    summary_file = batch_dir / \"analysis_summary.json\"\n",
    "    if summary_file.exists():\n",
    "        with open(summary_file, 'r') as f:\n",
    "            analysis_results['summary'] = json.load(f)\n",
    "        print(f\"   âœ… Summary: analysis_summary.json\")\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ QUALITY ANALYSIS RESULTS READY!\")\n",
    "    if 'summary' in analysis_results:\n",
    "        summary = analysis_results['summary']\n",
    "        print(f\"   ðŸ“Š Quality Score: {summary['quality_summary']['overall_score']:.3f}\")\n",
    "        print(f\"   ðŸ” Contaminated Fields: {summary['quality_summary']['contaminated_fields']}\")\n",
    "        print(f\"   ðŸ”§ Recommendations: {summary['completeness_summary']['total_recommendations']}\")\n",
    "\n",
    "    return analysis_results\n",
    "\n",
    "# Function to list available batches\n",
    "def list_available_batches():\n",
    "    \"\"\"List all available quality analysis batches.\"\"\"\n",
    "    results_dir = Path(\"../../data/quality_analysis\")\n",
    "    index_file = results_dir / \"batch_index.json\"\n",
    "\n",
    "    if not index_file.exists():\n",
    "        print(\"ðŸ“‚ No batch index found\")\n",
    "        return []\n",
    "\n",
    "    with open(index_file, 'r') as f:\n",
    "        batch_index = json.load(f)\n",
    "\n",
    "    print(\"AVAILABLE QUALITY ANALYSIS BATCHES\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    for i, batch in enumerate(batch_index['batches'][:10]):  # Show last 10\n",
    "        print(f\"{i+1:2d}. {batch['batch_id']}\")\n",
    "        print(f\"     ðŸ“… {batch['datetime'][:19]}\")\n",
    "        print(f\"     ðŸ“Š Quality: {batch['summary_scores']['overall_quality']:.3f}\")\n",
    "        print(f\"     ðŸ”§ Recommendations: {batch['summary_scores']['total_recommendations']}\")\n",
    "        print()\n",
    "\n",
    "    return batch_index['batches']\n",
    "\n",
    "# Load latest results by default\n",
    "quality_results = load_quality_analysis_results()\n",
    "\n",
    "# Optionally list available batches\n",
    "available_batches = list_available_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc17eeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CLEANING PRIORITY ANALYSIS\n",
      "==================================================\n",
      "SUMMARY FROM QUALITY ANALYSIS:\n",
      "   Overall Quality Score: 0.988\n",
      "   Contaminated Fields: 7\n",
      "   Critical Issues: 0\n",
      "\n",
      "CLEANING RECOMMENDATIONS:\n",
      "   Total Recommendations: 2\n",
      "   Critical Imputation Needed: 0\n",
      "   Low-value Fields: 2\n",
      "\n",
      "CONTAMINATION ISSUES BY DATASET:\n",
      "   business_licenses: 1 contamination issues\n",
      "   building_permits: 3 contamination issues\n",
      "\n",
      "READY TO BEGIN TARGETED DATA CLEANING!\n",
      "   High Priority Actions: 1\n",
      "   Critical Fixes: 0\n"
     ]
    }
   ],
   "source": [
    "# Extract key insights from quality analysis for cleaning priorities\n",
    "def analyze_cleaning_priorities(quality_results):\n",
    "    \"\"\"Extract actionable cleaning priorities from quality analysis.\"\"\"\n",
    "    print(\"DATA CLEANING PRIORITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if not quality_results:\n",
    "        print(\"ERROR: No quality results available\")\n",
    "        return None\n",
    "\n",
    "    summary = quality_results.get('summary', {})\n",
    "    contamination = quality_results.get('contamination', {})\n",
    "    completeness = quality_results.get('completeness', {})\n",
    "\n",
    "    # Extract high-priority cleaning actions\n",
    "    cleaning_plan = {\n",
    "        'critical_fixes': [],\n",
    "        'high_priority': [],\n",
    "        'medium_priority': [],\n",
    "        'field_specific_actions': {}\n",
    "    }\n",
    "\n",
    "    print(f\"SUMMARY FROM QUALITY ANALYSIS:\")\n",
    "    if 'quality_summary' in summary:\n",
    "        qs = summary['quality_summary']\n",
    "        print(f\"   Overall Quality Score: {qs.get('overall_score', 0):.3f}\")\n",
    "        print(f\"   Contaminated Fields: {qs.get('contaminated_fields', 0)}\")\n",
    "        print(f\"   Critical Issues: {qs.get('critical_issues', 0)}\")\n",
    "\n",
    "    print(f\"\\nCLEANING RECOMMENDATIONS:\")\n",
    "    if 'completeness_summary' in summary:\n",
    "        cs = summary['completeness_summary']\n",
    "        print(f\"   Total Recommendations: {cs.get('total_recommendations', 0)}\")\n",
    "        print(f\"   Critical Imputation Needed: {cs.get('critical_imputation_needed', 0)}\")\n",
    "        print(f\"   Low-value Fields: {cs.get('low_value_fields', 0)}\")\n",
    "\n",
    "    # Extract contamination issues for each dataset\n",
    "    print(f\"\\nCONTAMINATION ISSUES BY DATASET:\")\n",
    "    for dataset_name, contamination_info in contamination.items():\n",
    "        if isinstance(contamination_info, dict):\n",
    "            total_issues = (\n",
    "                len(contamination_info.get('mixed_types', [])) +\n",
    "                len(contamination_info.get('special_chars', [])) +\n",
    "                len(contamination_info.get('encoding_issues', [])) +\n",
    "                len(contamination_info.get('format_inconsistencies', []))\n",
    "            )\n",
    "            if total_issues > 0:\n",
    "                print(f\"   {dataset_name}: {total_issues} contamination issues\")\n",
    "\n",
    "                # Extract specific recommendations\n",
    "                recommendations = contamination_info.get('recommendations', [])\n",
    "                for rec in recommendations:\n",
    "                    cleaning_plan['high_priority'].append({\n",
    "                        'dataset': dataset_name,\n",
    "                        'action': rec,\n",
    "                        'type': 'contamination_fix'\n",
    "                    })\n",
    "\n",
    "    return cleaning_plan\n",
    "\n",
    "# Analyze cleaning priorities from our quality results\n",
    "cleaning_priorities = analyze_cleaning_priorities(quality_results)\n",
    "\n",
    "print(f\"\\nREADY TO BEGIN TARGETED DATA CLEANING!\")\n",
    "if cleaning_priorities:\n",
    "    print(f\"   High Priority Actions: {len(cleaning_priorities['high_priority'])}\")\n",
    "    print(f\"   Critical Fixes: {len(cleaning_priorities['critical_fixes'])}\")\n",
    "else:\n",
    "    print(\"   Will proceed with standard cleaning workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d435769",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4777447",
   "metadata": {},
   "source": [
    "### Priority-Based Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "135c108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Data Cleaning Based on Quality Analysis\n",
    "def create_cleaning_pipeline(quality_results, datasets):\n",
    "    \"\"\"\n",
    "    Create a dynamic cleaning pipeline based on quality analysis results.\n",
    "    Returns a list of cleaning functions to execute in priority order.\n",
    "    \"\"\"\n",
    "    print(\"CREATING DYNAMIC CLEANING PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    cleaning_steps = []\n",
    "\n",
    "    if not quality_results:\n",
    "        print(\"No quality results - using standard cleaning only\")\n",
    "        cleaning_steps.append(('standard_cleaning', standard_data_cleaning))\n",
    "        return cleaning_steps\n",
    "\n",
    "    contamination = quality_results.get('contamination', {})\n",
    "    completeness = quality_results.get('completeness', {})\n",
    "    summary = quality_results.get('summary', {})\n",
    "\n",
    "    # 1. CRITICAL FIXES FIRST (data integrity issues)\n",
    "    if summary.get('business_rules_summary', {}).get('total_violations', 0) > 0:\n",
    "        cleaning_steps.append(('fix_business_logic', fix_business_logic_issues))\n",
    "\n",
    "    # 2. HIGH PRIORITY: Contamination fixes (mixed types, special chars)\n",
    "    for dataset_name, contamination_info in contamination.items():\n",
    "        if isinstance(contamination_info, dict):\n",
    "            recommendations = contamination_info.get('recommendations', [])\n",
    "            if recommendations:\n",
    "                # FIX: Create proper closure function\n",
    "                def make_contamination_fixer(ds_name, recs):\n",
    "                    def contamination_fixer(datasets):\n",
    "                        return fix_contamination_issues(ds_name, recs, datasets)\n",
    "                    return contamination_fixer\n",
    "\n",
    "                cleaning_steps.append((f'fix_contamination_{dataset_name}',\n",
    "                                     make_contamination_fixer(dataset_name, recommendations)))\n",
    "\n",
    "    # 3. MEDIUM PRIORITY: Type conversions and standardization\n",
    "    cleaning_steps.append(('standardize_types', standardize_data_types))\n",
    "\n",
    "    # 4. LOW PRIORITY: Optional field cleanup\n",
    "    if summary.get('completeness_summary', {}).get('low_value_fields', 0) > 0:\n",
    "        cleaning_steps.append(('clean_optional_fields', clean_optional_fields))\n",
    "\n",
    "    # 5. ALWAYS: Final validation\n",
    "    cleaning_steps.append(('final_validation', validate_cleaned_data))\n",
    "\n",
    "    print(f\"Created pipeline with {len(cleaning_steps)} cleaning steps:\")\n",
    "    for i, (step_name, _) in enumerate(cleaning_steps, 1):\n",
    "        print(f\"  {i}. {step_name}\")\n",
    "\n",
    "    return cleaning_steps\n",
    "\n",
    "def execute_cleaning_pipeline(cleaning_steps, datasets):\n",
    "    \"\"\"Execute the cleaning pipeline and return cleaned datasets.\"\"\"\n",
    "    print(f\"\\nEXECUTING CLEANING PIPELINE\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    cleaned_datasets = datasets.copy()\n",
    "\n",
    "    for step_name, cleaning_func in cleaning_steps:\n",
    "        print(f\"\\nRunning: {step_name}\")\n",
    "        try:\n",
    "            result = cleaning_func(cleaned_datasets)\n",
    "            if result:\n",
    "                cleaned_datasets = result\n",
    "            print(f\"  COMPLETED: {step_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR in {step_name}: {str(e)}\")\n",
    "            print(f\"  Continuing with next step...\")\n",
    "\n",
    "    return cleaned_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99098e08",
   "metadata": {},
   "source": [
    "### Core Cleaning Functions (Simple & Targeted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa9540f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fix contamination issues dynamically\n",
    "def fix_contamination_issues(dataset_name, recommendations, datasets):\n",
    "    \"\"\"Apply contamination fixes based on quality analysis recommendations.\"\"\"\n",
    "    if dataset_name not in datasets:\n",
    "        return datasets\n",
    "\n",
    "    df = datasets[dataset_name].copy()\n",
    "    print(f\"    Fixing contamination in {dataset_name}\")\n",
    "\n",
    "    for recommendation in recommendations:\n",
    "        try:\n",
    "            # Parse the recommendation to extract field and operation\n",
    "            if 'pd.to_numeric' in recommendation:\n",
    "                # Extract field name from recommendation like \"Convert zip_code to numeric: pd.to_numeric(df['zip_code'], errors='coerce')\"\n",
    "                field_start = recommendation.find(\"df['\") + 4\n",
    "                field_end = recommendation.find(\"']\", field_start)\n",
    "                field_name = recommendation[field_start:field_end]\n",
    "\n",
    "                # Apply numeric conversion\n",
    "                df[field_name] = pd.to_numeric(df[field_name], errors='coerce')\n",
    "                print(f\"      Applied: Convert {field_name} to numeric\")\n",
    "\n",
    "            elif 'str.replace' in recommendation:\n",
    "                # Extract field and pattern from recommendations like \"Strip quotes from zip_code: df['zip_code'].str.replace(r'[\\'\"]', '', regex=True)\"\n",
    "                field_start = recommendation.find(\"df['\") + 4\n",
    "                field_end = recommendation.find(\"']\", field_start)\n",
    "                field_name = recommendation[field_start:field_end]\n",
    "\n",
    "                # Extract pattern and replacement\n",
    "                if \"str.replace(r'[\\\\'\\\\\\\"]'\" in recommendation:\n",
    "                    # Remove quotes\n",
    "                    df[field_name] = df[field_name].astype(str).str.replace(r'[\\'\"]', '', regex=True)\n",
    "                    print(f\"      Applied: Strip quotes from {field_name}\")\n",
    "                else:\n",
    "                    # Generic string replacement\n",
    "                    df[field_name] = df[field_name].astype(str).str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "                    print(f\"      Applied: Clean special characters from {field_name}\")\n",
    "\n",
    "            elif 'str.strip' in recommendation:\n",
    "                # Extract field name\n",
    "                field_start = recommendation.find(\"df['\") + 4\n",
    "                field_end = recommendation.find(\"']\", field_start)\n",
    "                field_name = recommendation[field_start:field_end]\n",
    "\n",
    "                # Apply strip\n",
    "                df[field_name] = df[field_name].astype(str).str.strip()\n",
    "                print(f\"      Applied: Strip whitespace from {field_name}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"      Skipped: Unknown recommendation format\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      FAILED: {recommendation[:40]}... ({str(e)})\")\n",
    "\n",
    "    datasets[dataset_name] = df\n",
    "    return datasets\n",
    "\n",
    "# 2. Standardize data types based on schema\n",
    "def standardize_data_types(datasets):\n",
    "    \"\"\"Convert fields to proper types based on schema definitions.\"\"\"\n",
    "    print(\"    Standardizing data types using schema\")\n",
    "\n",
    "    for dataset_name, df in datasets.items():\n",
    "        schema_fields = SchemaManager.get_field_names(dataset_name)\n",
    "        date_fields = SchemaManager.get_date_fields(dataset_name)\n",
    "\n",
    "        # Convert date fields\n",
    "        for field in date_fields:\n",
    "            if field in df.columns:\n",
    "                try:\n",
    "                    df[field] = pd.to_datetime(df[field], errors='coerce')\n",
    "                    print(f\"      Converted {field} to datetime\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # Convert numeric fields that should be numeric\n",
    "        numeric_candidates = ['community_area', 'ward', 'precinct', 'zip_code', 'latitude', 'longitude']\n",
    "        for field in numeric_candidates:\n",
    "            if field in df.columns:\n",
    "                try:\n",
    "                    df[field] = pd.to_numeric(df[field], errors='coerce')\n",
    "                    print(f\"      Converted {field} to numeric\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        datasets[dataset_name] = df\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# 3. Clean optional fields with low value\n",
    "def clean_optional_fields(datasets):\n",
    "    \"\"\"Handle optional fields with very low completion rates.\"\"\"\n",
    "    print(\"    Cleaning optional fields\")\n",
    "\n",
    "    for dataset_name, df in datasets.items():\n",
    "        optional_fields = set(SchemaManager.get_field_names(dataset_name)) - set(SchemaManager.get_required_fields(dataset_name))\n",
    "\n",
    "        for field in optional_fields:\n",
    "            if field in df.columns:\n",
    "                completion_rate = df[field].notna().sum() / len(df)\n",
    "\n",
    "                # Drop fields with <5% completion\n",
    "                if completion_rate < 0.05:\n",
    "                    print(f\"      Dropping {field} ({completion_rate:.1%} complete)\")\n",
    "                    df = df.drop(columns=[field])\n",
    "\n",
    "                # Fill fields with 5-25% completion with 'UNKNOWN'\n",
    "                elif completion_rate < 0.25 and df[field].dtype == 'object':\n",
    "                    df[field] = df[field].fillna('UNKNOWN')\n",
    "                    print(f\"      Filled {field} nulls with 'UNKNOWN'\")\n",
    "\n",
    "        datasets[dataset_name] = df\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# 4. Fix business logic issues\n",
    "def fix_business_logic_issues(datasets):\n",
    "    \"\"\"Fix basic business logic violations.\"\"\"\n",
    "    print(\"    Fixing business logic issues\")\n",
    "\n",
    "    # Example: Fix date logic issues in licenses\n",
    "    if 'business_licenses' in datasets:\n",
    "        df = datasets['business_licenses'].copy()\n",
    "\n",
    "        # Fix licenses where start date > expiration date\n",
    "        if 'license_start_date' in df.columns and 'expiration_date' in df.columns:\n",
    "            invalid_dates = df['license_start_date'] > df['expiration_date']\n",
    "            if invalid_dates.sum() > 0:\n",
    "                # Set expiration to start date + 1 year for invalid cases\n",
    "                df.loc[invalid_dates, 'expiration_date'] = df.loc[invalid_dates, 'license_start_date'] + pd.DateOffset(years=1)\n",
    "                print(f\"      Fixed {invalid_dates.sum()} invalid date sequences\")\n",
    "\n",
    "        datasets['business_licenses'] = df\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# 5. Standard cleaning (always applied)\n",
    "def standard_data_cleaning(datasets):\n",
    "    \"\"\"Apply standard cleaning operations to all datasets.\"\"\"\n",
    "    print(\"    Applying standard cleaning\")\n",
    "\n",
    "    for dataset_name, df in datasets.items():\n",
    "        # Remove completely empty rows\n",
    "        before_rows = len(df)\n",
    "        df = df.dropna(how='all')\n",
    "        if len(df) < before_rows:\n",
    "            print(f\"      Removed {before_rows - len(df)} empty rows from {dataset_name}\")\n",
    "\n",
    "        # Strip whitespace from text fields\n",
    "        text_fields = df.select_dtypes(include=['object']).columns\n",
    "        for field in text_fields:\n",
    "            df[field] = df[field].astype(str).str.strip()\n",
    "\n",
    "        datasets[dataset_name] = df\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# 6. Final validation\n",
    "def validate_cleaned_data(datasets):\n",
    "    \"\"\"Validate the cleaned data meets basic requirements.\"\"\"\n",
    "    print(\"    Validating cleaned data\")\n",
    "\n",
    "    for dataset_name, df in datasets.items():\n",
    "        required_fields = SchemaManager.get_required_fields(dataset_name)\n",
    "\n",
    "        # Check required fields still exist and aren't empty\n",
    "        for field in required_fields:\n",
    "            if field not in df.columns:\n",
    "                print(f\"      WARNING: Required field {field} missing from {dataset_name}\")\n",
    "            elif df[field].isna().all():\n",
    "                print(f\"      WARNING: Required field {field} is completely empty in {dataset_name}\")\n",
    "\n",
    "        print(f\"      {dataset_name}: {len(df)} rows, {len(df.columns)} columns after cleaning\")\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9c577",
   "metadata": {},
   "source": [
    "### Simple Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "236b6a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING DATA CLEANING PROCESS\n",
      "==================================================\n",
      "CREATING DYNAMIC CLEANING PIPELINE\n",
      "==================================================\n",
      "Created pipeline with 5 cleaning steps:\n",
      "  1. fix_business_logic\n",
      "  2. fix_contamination_business_licenses\n",
      "  3. standardize_types\n",
      "  4. clean_optional_fields\n",
      "  5. final_validation\n",
      "\n",
      "EXECUTING CLEANING PIPELINE\n",
      "========================================\n",
      "\n",
      "Running: fix_business_logic\n",
      "    Fixing business logic issues\n",
      "      Fixed 67 invalid date sequences\n",
      "  COMPLETED: fix_business_logic\n",
      "\n",
      "Running: fix_contamination_business_licenses\n",
      "    Fixing contamination in business_licenses\n",
      "      Applied: Convert zip_code to numeric\n",
      "  COMPLETED: fix_contamination_business_licenses\n",
      "\n",
      "Running: standardize_types\n",
      "    Standardizing data types using schema\n",
      "      Converted application_created_date to datetime\n",
      "      Converted application_requirements_complete to datetime\n",
      "      Converted payment_date to datetime\n",
      "      Converted conditional_approval to datetime\n",
      "      Converted license_approved_for_issuance to datetime\n",
      "      Converted date_issued to datetime\n",
      "      Converted license_start_date to datetime\n",
      "      Converted expiration_date to datetime\n",
      "      Converted license_status_change_date to datetime\n",
      "      Converted community_area to numeric\n",
      "      Converted ward to numeric\n",
      "      Converted precinct to numeric\n",
      "      Converted zip_code to numeric\n",
      "      Converted latitude to numeric\n",
      "      Converted longitude to numeric\n",
      "      Converted application_start_date to datetime\n",
      "      Converted issue_date to datetime\n",
      "      Converted community_area to numeric\n",
      "      Converted service_date to datetime\n",
      "  COMPLETED: standardize_types\n",
      "\n",
      "Running: clean_optional_fields\n",
      "    Cleaning optional fields\n",
      "      Dropping license_status_change_date (0.4% complete)\n",
      "      Dropping conditional_approval (0.0% complete)\n",
      "  COMPLETED: clean_optional_fields\n",
      "\n",
      "Running: final_validation\n",
      "    Validating cleaned data\n",
      "      business_licenses: 2040 rows, 37 columns after cleaning\n",
      "      building_permits: 8647 rows, 31 columns after cleaning\n",
      "      cta_boardings: 668 rows, 2 columns after cleaning\n",
      "  COMPLETED: final_validation\n",
      "\n",
      "DATA CLEANING COMPLETE!\n",
      "  Business Licenses: 2040 rows\n",
      "  Building Permits: 8647 rows\n",
      "  CTA Boardings: 668 rows\n"
     ]
    }
   ],
   "source": [
    "# Execute the dynamic cleaning pipeline\n",
    "def run_data_cleaning():\n",
    "    \"\"\"Main function to run the complete data cleaning pipeline.\"\"\"\n",
    "    print(\"STARTING DATA CLEANING PROCESS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create dynamic pipeline based on quality analysis\n",
    "    cleaning_pipeline = create_cleaning_pipeline(quality_results, datasets)\n",
    "\n",
    "    # Execute the pipeline\n",
    "    cleaned_datasets = execute_cleaning_pipeline(cleaning_pipeline, datasets)\n",
    "\n",
    "    # Extract cleaned dataframes with descriptive names\n",
    "    licenses_df_cleaned = cleaned_datasets['business_licenses']\n",
    "    permits_df_cleaned = cleaned_datasets['building_permits']\n",
    "    cta_df_cleaned = cleaned_datasets['cta_boardings']\n",
    "\n",
    "    print(f\"\\nDATA CLEANING COMPLETE!\")\n",
    "    print(f\"  Business Licenses: {len(licenses_df_cleaned)} rows\")\n",
    "    print(f\"  Building Permits: {len(permits_df_cleaned)} rows\")\n",
    "    print(f\"  CTA Boardings: {len(cta_df_cleaned)} rows\")\n",
    "\n",
    "    return licenses_df_cleaned, permits_df_cleaned, cta_df_cleaned\n",
    "\n",
    "# Run the cleaning process\n",
    "licenses_df_cleaned, permits_df_cleaned, cta_df_cleaned = run_data_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a8fe28",
   "metadata": {},
   "source": [
    "# Data Quality Verification\n",
    "\n",
    "We can trust, but need to verify that our data is ready to ship out for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587c338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEFORE vs AFTER CLEANING COMPARISON\n",
      "============================================================\n",
      "\n",
      "BUSINESS LICENSES\n",
      "----------------------------------------\n",
      "  Rows:           2,040 â†’ 2,040\n",
      "  Columns:        39 â†’ 37\n",
      "  Numeric Fields: 5 â†’ 11\n",
      "  Date Fields:    9 â†’ 7\n",
      "  Required Nulls: 0 â†’ 244\n",
      "\n",
      "BUILDING PERMITS\n",
      "----------------------------------------\n",
      "  Rows:           8,647 â†’ 8,647\n",
      "  Columns:        31 â†’ 31\n",
      "  Numeric Fields: 16 â†’ 17\n",
      "  Date Fields:    2 â†’ 2\n",
      "  Required Nulls: 0 â†’ 0\n",
      "\n",
      "CTA BOARDINGS\n",
      "----------------------------------------\n",
      "  Rows:           668 â†’ 668\n",
      "  Columns:        2 â†’ 2\n",
      "  Numeric Fields: 1 â†’ 1\n",
      "  Date Fields:    1 â†’ 1\n",
      "  Required Nulls: 0 â†’ 0\n"
     ]
    }
   ],
   "source": [
    "# Cell: Before/After Comparison\n",
    "def compare_before_after_cleaning():\n",
    "    \"\"\"Compare data quality metrics before and after cleaning.\"\"\"\n",
    "    print(\"\\nBEFORE vs AFTER CLEANING COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    datasets_comparison = {\n",
    "        'business_licenses': (licenses_df, licenses_df_cleaned),\n",
    "        'building_permits': (permits_df, permits_df_cleaned),\n",
    "        'cta_boardings': (cta_df, cta_df_cleaned)\n",
    "    }\n",
    "\n",
    "    for dataset_name, (original_df, cleaned_df) in datasets_comparison.items():\n",
    "        print(f\"\\n{dataset_name.upper().replace('_', ' ')}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Basic metrics\n",
    "        print(f\"  Rows:           {len(original_df):,} â†’ {len(cleaned_df):,}\")\n",
    "        print(f\"  Columns:        {len(original_df.columns)} â†’ {len(cleaned_df.columns)}\")\n",
    "\n",
    "        # Data type improvements\n",
    "        original_numeric = len(original_df.select_dtypes(include=[np.number]).columns)\n",
    "        cleaned_numeric = len(cleaned_df.select_dtypes(include=[np.number]).columns)\n",
    "        original_datetime = len(original_df.select_dtypes(include=['datetime']).columns)\n",
    "        cleaned_datetime = len(cleaned_df.select_dtypes(include=['datetime']).columns)\n",
    "\n",
    "        print(f\"  Numeric Fields: {original_numeric} â†’ {cleaned_numeric}\")\n",
    "        print(f\"  Date Fields:    {original_datetime} â†’ {cleaned_datetime}\")\n",
    "\n",
    "        # Required fields completeness\n",
    "        required_fields = SchemaManager.get_required_fields(dataset_name)\n",
    "        original_nulls = sum(original_df[field].isnull().sum() for field in required_fields if field in original_df.columns)\n",
    "        cleaned_nulls = sum(cleaned_df[field].isnull().sum() for field in required_fields if field in cleaned_df.columns)\n",
    "\n",
    "        print(f\"  Required Nulls: {original_nulls:,} â†’ {cleaned_nulls:,}\")\n",
    "\n",
    "compare_before_after_cleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688cb19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA READINESS VALIDATION\n",
      "========================================\n",
      "\n",
      "Business Licenses:\n",
      "  NULL Required Fields: ['community_area']\n",
      "  DATA TYPES: CORRECT\n",
      "  STATUS: NEEDS ATTENTION\n",
      "\n",
      "Building Permits:\n",
      "  REQUIRED Fields: ALL PRESENT AND COMPLETE\n",
      "  DATA TYPES: CORRECT\n",
      "  STATUS: READY FOR ANALYSIS\n",
      "\n",
      "Cta Boardings:\n",
      "  REQUIRED Fields: ALL PRESENT AND COMPLETE\n",
      "  DATA TYPES: CORRECT\n",
      "  STATUS: READY FOR ANALYSIS\n",
      "\n",
      "OVERALL STATUS: REQUIRES FIXES\n"
     ]
    }
   ],
   "source": [
    "# Cell: Final Validation & Readiness Check\n",
    "def validate_data_readiness():\n",
    "    \"\"\"Validate that data is ready for analysis and next steps.\"\"\"\n",
    "    print(\"\\nDATA READINESS VALIDATION\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    ready_for_analysis = True\n",
    "\n",
    "    datasets_to_validate = {\n",
    "        'business_licenses': licenses_df_cleaned,\n",
    "        'building_permits': permits_df_cleaned,\n",
    "        'cta_boardings': cta_df_cleaned\n",
    "    }\n",
    "\n",
    "    for dataset_name, df in datasets_to_validate.items():\n",
    "        print(f\"\\n{dataset_name.replace('_', ' ').title()}:\")\n",
    "\n",
    "        # Check required fields\n",
    "        required_fields = SchemaManager.get_required_fields(dataset_name)\n",
    "        missing_required = [field for field in required_fields if field not in df.columns]\n",
    "        null_required = [field for field in required_fields if field in df.columns and df[field].isnull().any()]\n",
    "\n",
    "        if missing_required:\n",
    "            print(f\"  MISSING Required Fields: {missing_required}\")\n",
    "            ready_for_analysis = False\n",
    "        elif null_required:\n",
    "            print(f\"  NULL Required Fields: {null_required}\")\n",
    "            ready_for_analysis = False\n",
    "        else:\n",
    "            print(f\"  REQUIRED Fields: ALL PRESENT AND COMPLETE\")\n",
    "\n",
    "        # Check data types\n",
    "        date_fields = SchemaManager.get_date_fields(dataset_name)\n",
    "        incorrect_types = [field for field in date_fields if field in df.columns and df[field].dtype != 'datetime64[ns]']\n",
    "\n",
    "        if incorrect_types:\n",
    "            print(f\"  INCORRECT Types: {incorrect_types}\")\n",
    "            ready_for_analysis = False\n",
    "        else:\n",
    "            print(f\"  DATA TYPES: CORRECT\")\n",
    "\n",
    "        print(f\"  STATUS: {'READY FOR ANALYSIS' if not (missing_required or null_required or incorrect_types) else 'NEEDS ATTENTION'}\")\n",
    "\n",
    "    print(f\"\\nOVERALL STATUS: {'READY FOR STEP 4 (LOAD & VALIDATE)' if ready_for_analysis else 'REQUIRES FIXES'}\")\n",
    "    return ready_for_analysis\n",
    "\n",
    "data_ready = validate_data_readiness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf1a9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXING COMMUNITY_AREA NULLS\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'licenses_df_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m licenses_df_cleaned\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Apply the fix\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m licenses_df_cleaned = \u001b[43mfix_community_area_nulls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Re-run validation to confirm fix\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRE-VALIDATION:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mfix_community_area_nulls\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m30\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Check what we're working with\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m null_count = \u001b[43mlicenses_df_cleaned\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mcommunity_area\u001b[39m\u001b[33m'\u001b[39m].isnull().sum()\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecords with null community_area: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnull_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m null_count == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mNameError\u001b[39m: name 'licenses_df_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "# Fix Cell: Handle community_area nulls intelligently\n",
    "def fix_community_area_nulls():\n",
    "    \"\"\"Fix community_area nulls using available geographic data.\"\"\"\n",
    "    print(\"FIXING COMMUNITY_AREA NULLS\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    # Check what we're working with\n",
    "    null_count = licenses_df_cleaned['community_area'].isnull().sum()\n",
    "    print(f\"Records with null community_area: {null_count}\")\n",
    "\n",
    "    if null_count == 0:\n",
    "        print(\"No nulls to fix!\")\n",
    "        return licenses_df_cleaned\n",
    "\n",
    "    # Strategy 1: Use community_area_name to fill missing community_area\n",
    "    if 'community_area_name' in licenses_df_cleaned.columns:\n",
    "        # Create mapping from community_area_name to community_area\n",
    "        area_mapping = licenses_df_cleaned[licenses_df_cleaned['community_area'].notna()].groupby('community_area_name')['community_area'].first()\n",
    "\n",
    "        # Fill nulls using the mapping\n",
    "        null_mask = licenses_df_cleaned['community_area'].isnull()\n",
    "        for idx in licenses_df_cleaned[null_mask].index:\n",
    "            area_name = licenses_df_cleaned.loc[idx, 'community_area_name']\n",
    "            if area_name in area_mapping:\n",
    "                licenses_df_cleaned.loc[idx, 'community_area'] = area_mapping[area_name]\n",
    "\n",
    "        fixed_count = null_count - licenses_df_cleaned['community_area'].isnull().sum()\n",
    "        print(f\"Fixed {fixed_count} nulls using community_area_name mapping\")\n",
    "\n",
    "    # Strategy 2: For remaining nulls, use a default value or remove from required list\n",
    "    remaining_nulls = licenses_df_cleaned['community_area'].isnull().sum()\n",
    "    if remaining_nulls > 0:\n",
    "        print(f\"Still have {remaining_nulls} nulls after mapping\")\n",
    "        print(\"Options:\")\n",
    "        print(\"1. Fill with default value (e.g., 0 for 'Unknown')\")\n",
    "        print(\"2. Remove community_area from required fields list\")\n",
    "        print(\"3. Drop these records (if very few)\")\n",
    "\n",
    "        # For now, let's fill with 0 (representing unknown area)\n",
    "        licenses_df_cleaned['community_area'] = licenses_df_cleaned['community_area'].fillna(0)\n",
    "        print(f\"Filled remaining {remaining_nulls} nulls with 0 (Unknown area)\")\n",
    "\n",
    "    return licenses_df_cleaned\n",
    "\n",
    "# Apply the fix\n",
    "licenses_df_cleaned = fix_community_area_nulls()\n",
    "\n",
    "# Re-run validation to confirm fix\n",
    "print(f\"\\nRE-VALIDATION:\")\n",
    "null_count_after = licenses_df_cleaned['community_area'].isnull().sum()\n",
    "print(f\"Remaining nulls in community_area: {null_count_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7bd7f4",
   "metadata": {},
   "source": [
    "# Load data in Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc85a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA NOT READY - Fix validation issues before saving to Google Sheets\n",
      "Available options:\n",
      "1. Fix the community_area nulls and re-validate\n",
      "2. Save anyway for manual review in Google Sheets\n"
     ]
    }
   ],
   "source": [
    "# # Cell: Save Cleaned Data to Google Sheets\n",
    "# def save_cleaned_data_to_sheets():\n",
    "#     \"\"\"Save cleaned datasets to Google Sheets with '_Cleaned' suffix.\"\"\"\n",
    "#     print(\"SAVING CLEANED DATA TO GOOGLE SHEETS\")\n",
    "#     print(\"=\" * 50)\n",
    "\n",
    "#     # Load Google Sheets connection\n",
    "#     settings = load_settings()\n",
    "#     sh = open_sheet(settings.sheet_id, settings.google_creds_path)\n",
    "\n",
    "#     # Define cleaned datasets and their target worksheet names\n",
    "#     cleaned_data_mapping = {\n",
    "#         'Business_Licenses_Cleaned': licenses_df_cleaned,\n",
    "#         'Building_Permits_Cleaned': permits_df_cleaned,\n",
    "#         'CTA_Cleaned': cta_df_cleaned\n",
    "#     }\n",
    "\n",
    "#     for worksheet_name, df in cleaned_data_mapping.items():\n",
    "#         try:\n",
    "#             print(f\"\\nSaving {worksheet_name}...\")\n",
    "#             print(f\"  Rows: {len(df):,}\")\n",
    "#             print(f\"  Columns: {len(df.columns)}\")\n",
    "\n",
    "#             # Create or update worksheet with cleaned data\n",
    "#             upsert_worksheet(sh, worksheet_name)\n",
    "#             overwrite_with_dataframe(sh, worksheet_name, df)\n",
    "\n",
    "#             print(f\"  SUCCESS: Saved to '{worksheet_name}' tab\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"  ERROR saving {worksheet_name}: {str(e)}\")\n",
    "\n",
    "#     print(f\"\\nCLEANED DATA SAVED TO GOOGLE SHEETS!\")\n",
    "#     print(f\"   Original tabs: Business_Licenses_Full, Building_Permits_Full, CTA_Full\")\n",
    "#     print(f\"   Cleaned tabs:  Business_Licenses_Cleaned, Building_Permits_Cleaned, CTA_Cleaned\")\n",
    "\n",
    "#     return True\n",
    "\n",
    "# # Save to Google Sheets\n",
    "# if data_ready:\n",
    "#     save_success = save_cleaned_data_to_sheets()\n",
    "# else:\n",
    "#     print(\"\\nDATA NOT READY - Fix validation issues before saving to Google Sheets\")\n",
    "#     print(\"Available options:\")\n",
    "#     print(\"1. Fix the community_area nulls and re-validate\")\n",
    "#     print(\"2. Save anyway for manual review in Google Sheets\")\n",
    "\n",
    "#     # Option to save anyway for review\n",
    "#     save_anyway = input(\"Save to Google Sheets anyway for manual review? (y/n): \")\n",
    "#     if save_anyway.lower() == 'y':\n",
    "#         save_success = save_cleaned_data_to_sheets()\n",
    "#         print(\"WARNING: Data saved but requires manual review and fixes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576eb8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
