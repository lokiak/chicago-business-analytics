{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning with Great Expectations Integration\n",
        "\n",
        "This notebook demonstrates the new Great Expectations-powered data cleaning workflow that replaces manual cleaning with automated, validated transformations.\n",
        "\n",
        "## Key Benefits:\n",
        "- **Automated field type detection** based on naming patterns\n",
        "- **Chicago-specific business rules** and validation\n",
        "- **Comprehensive quality reporting** with 60+ validation checks\n",
        "- **Fallback reliability** to manual cleaning if needed\n",
        "- **Drop-in replacement** for existing workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "# Add paths for our custom modules\n",
        "sys.path.append('../../shared')\n",
        "sys.path.append('../../step2_data_ingestion')\n",
        "sys.path.append('../')  # For step3_transform_model modules\n",
        "\n",
        "# Import existing modules\n",
        "from sheets_client import open_sheet\n",
        "from config_manager import load_settings\n",
        "from schema import SchemaManager\n",
        "from notebook_utils import *\n",
        "\n",
        "print(\"‚úÖ Standard imports successful\")\n",
        "\n",
        "# Import GX modules\n",
        "try:\n",
        "    from gx_data_cleaning import SmartDataCleaner, batch_clean_datasets\n",
        "    from desired_schema import DesiredSchemaManager, FieldTypeDetector\n",
        "    from expectation_suites import ChicagoSMBExpectationSuites\n",
        "    from pipeline_integration import enhanced_clean_and_save, compare_cleaning_methods\n",
        "    GX_AVAILABLE = True\n",
        "    print(\"‚úÖ Great Expectations modules imported successfully\")\n",
        "\n",
        "    import great_expectations as gx\n",
        "    print(f\"‚úÖ Great Expectations {gx.__version__} available\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå GX module import error: {e}\")\n",
        "    print(\"   Falling back to manual cleaning workflow\")\n",
        "    GX_AVAILABLE = False\n",
        "\n",
        "print(f\"\\nüéØ Setup Status:\")\n",
        "print(f\"   GX Integration: {'‚úÖ Ready' if GX_AVAILABLE else '‚ùå Manual Fallback'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Raw Data from Google Sheets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load original datasets (same loading logic as before)\n",
        "datasets_config = {\n",
        "    'business_licenses': {\n",
        "        'worksheet': 'Business_Licenses_Full',\n",
        "        'pickle_name': 'licenses_df'\n",
        "    },\n",
        "    'building_permits': {\n",
        "        'worksheet': 'Building_Permits_Full',\n",
        "        'pickle_name': 'permits_df'\n",
        "    },\n",
        "    'cta_boardings': {\n",
        "        'worksheet': 'CTA_Full',\n",
        "        'pickle_name': 'cta_df'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Load datasets from cache first, fall back to sheets\n",
        "datasets = {}\n",
        "load_from_sheets = False\n",
        "\n",
        "print(\"üìä LOADING RAW DATA FOR CLEANING...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for dataset_name, config in datasets_config.items():\n",
        "    try:\n",
        "        df = load_analysis_results(config['pickle_name'])\n",
        "        if df.empty:\n",
        "            raise FileNotFoundError(f\"{config['pickle_name']} is empty\")\n",
        "        datasets[dataset_name] = df\n",
        "        print(f\"   üì¶ LOADED {dataset_name}: {len(df):,} rows from cache\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"   üì• CACHE MISS {dataset_name}: will load from sheets\")\n",
        "        load_from_sheets = True\n",
        "\n",
        "if load_from_sheets:\n",
        "    print(\"\\nüîÑ Loading fresh data from Google Sheets...\")\n",
        "    settings = load_settings()\n",
        "    sh = open_sheet(settings.sheet_id, settings.google_creds_path)\n",
        "\n",
        "    for dataset_name, config in datasets_config.items():\n",
        "        if dataset_name not in datasets:\n",
        "            df = load_sheet_data(sh, config['worksheet'])\n",
        "            datasets[dataset_name] = df\n",
        "            save_analysis_results(df, config['pickle_name'])\n",
        "            print(f\"   üìä LOADED {dataset_name}: {len(df):,} rows from sheets and cached\")\n",
        "\n",
        "print(f\"\\n‚úÖ RAW DATA LOADED SUCCESSFULLY\")\n",
        "total_records = sum(len(df) for df in datasets.values())\n",
        "for name, df in datasets.items():\n",
        "    print(f\"   {name}: {len(df):,} rows, {len(df.columns)} columns\")\n",
        "print(f\"   üìà Total records: {total_records:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Great Expectations Data Cleaning\n",
        "\n",
        "This replaces the entire manual cleaning workflow with automated, validated transformations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if GX_AVAILABLE:\n",
        "    print(\"üöÄ ENHANCED DATA CLEANING WITH GREAT EXPECTATIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ONE-LINE REPLACEMENT for all manual cleaning!\n",
        "    cleaned_datasets, cleaning_report = enhanced_clean_and_save(datasets, use_gx=True)\n",
        "\n",
        "    # Extract cleaned dataframes (same variable names as manual workflow)\n",
        "    licenses_df_cleaned = cleaned_datasets['business_licenses']\n",
        "    permits_df_cleaned = cleaned_datasets['building_permits']\n",
        "    cta_df_cleaned = cleaned_datasets['cta_boardings']\n",
        "\n",
        "    print(f\"\\n‚úÖ GX CLEANING COMPLETE!\")\n",
        "    print(f\"   Strategy used: {cleaning_report['strategy_used']}\")\n",
        "    print(f\"   Datasets processed: {len(cleaning_report['datasets_processed'])}\")\n",
        "    print(f\"   Errors: {len(cleaning_report['errors'])}\")\n",
        "    print(f\"   Google Sheets saved: {cleaning_report['save_success']}\")\n",
        "\n",
        "    # Show detailed results\n",
        "    print(f\"\\nüìä CLEANING RESULTS BY DATASET:\")\n",
        "    for dataset_result in cleaning_report['datasets_processed']:\n",
        "        name = dataset_result['name']\n",
        "        success = \"‚úÖ\" if dataset_result['success'] else \"‚ùå\"\n",
        "        original_shape = dataset_result['original_shape']\n",
        "        cleaned_shape = dataset_result['cleaned_shape']\n",
        "        print(f\"   {success} {name}: {original_shape} ‚Üí {cleaned_shape}\")\n",
        "\n",
        "    # Show validation results if available\n",
        "    validation_results = cleaning_report.get('validation_results', {})\n",
        "    if validation_results:\n",
        "        print(f\"\\nüîç VALIDATION RESULTS:\")\n",
        "        for dataset_name, val_result in validation_results.items():\n",
        "            if 'success_rate' in val_result:\n",
        "                rate = val_result['success_rate']\n",
        "                total = val_result.get('total_expectations', 0)\n",
        "                print(f\"   {dataset_name}: {rate:.1%} success rate ({total} expectations)\")\n",
        "\n",
        "    # Show quality improvements\n",
        "    quality_improvements = cleaning_report.get('quality_improvements', {})\n",
        "    if quality_improvements:\n",
        "        print(f\"\\nüéØ QUALITY IMPROVEMENTS:\")\n",
        "        for dataset_name, improvements in quality_improvements.items():\n",
        "            if 'data_types' in improvements:\n",
        "                dt = improvements['data_types']\n",
        "                orig_numeric = dt['original_numeric']\n",
        "                clean_numeric = dt['cleaned_numeric']\n",
        "                orig_datetime = dt['original_datetime']\n",
        "                clean_datetime = dt['cleaned_datetime']\n",
        "\n",
        "                print(f\"   {dataset_name}:\")\n",
        "                print(f\"      Numeric fields: {orig_numeric} ‚Üí {clean_numeric}\")\n",
        "                print(f\"      DateTime fields: {orig_datetime} ‚Üí {clean_datetime}\")\n",
        "\n",
        "    print(f\"\\nüéâ AUTOMATED CLEANING COMPLETE!\")\n",
        "    print(f\"   üìã Check Google Sheets tabs ending in '_GX_Cleaned'\")\n",
        "    print(f\"   üìä All datasets are now analysis-ready\")\n",
        "\n",
        "    # Set flag for rest of notebook\n",
        "    data_ready = True\n",
        "    cleaning_method = \"Great Expectations\"\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  GX not available - would fall back to manual cleaning\")\n",
        "    print(\"   (Manual cleaning cells would go here)\")\n",
        "    data_ready = False\n",
        "    cleaning_method = \"Manual Fallback\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Next Steps\n",
        "\n",
        "Your cleaned data is now ready for analysis! The datasets are available as:\n",
        "- `licenses_df_cleaned` - Business licenses with validated fields\n",
        "- `permits_df_cleaned` - Building permits with proper data types  \n",
        "- `cta_df_cleaned` - CTA ridership data with temporal validation\n",
        "\n",
        "**Google Sheets tabs created:**\n",
        "- `Business_Licenses_GX_Cleaned`\n",
        "- `Building_Permits_GX_Cleaned` \n",
        "- `CTA_GX_Cleaned`\n",
        "\n",
        "**What Great Expectations provided:**\n",
        "- ‚úÖ Automated field type detection and conversion\n",
        "- ‚úÖ Chicago-specific business rule validation\n",
        "- ‚úÖ Comprehensive quality checks (60+ expectations)\n",
        "- ‚úÖ Detailed validation reporting\n",
        "- ‚úÖ Reliable fallback to manual cleaning if needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
