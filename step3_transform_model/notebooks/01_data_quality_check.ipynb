{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chicago SMB Market Radar ‚Äî Business Analysis\n",
        "\n",
        "This notebook will be where we clean our data in preparation for analysis\n",
        "\n",
        "## Objectives\n",
        "- Load data\n",
        "- Perform EDA on the data\n",
        "- Clean up the data for missing values, mixed datatypes by column, etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Imports successful\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add paths for correct imports\n",
        "import sys\n",
        "sys.path.append('../../shared')  # For shared utilities\n",
        "sys.path.append('../../step2_data_ingestion')  # For config and data access\n",
        "\n",
        "# Import our utility functions\n",
        "sys.path.append('../../shared')\n",
        "from notebook_utils import *\n",
        "\n",
        "# Import our custom modules\n",
        "from sheets_client import open_sheet\n",
        "from config_manager import load_settings\n",
        "from schema import SchemaManager\n",
        "\n",
        "print(\"‚úÖ Imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n",
        "\n",
        "Load the datasets from Google Sheets or from saved pickle files if available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Checking for cached data...\n",
            "‚úÖ Loaded analysis results from ../data/processed/licenses_df.pkl\n",
            "   ‚úÖ business_licenses: 2040 rows from cache\n",
            "‚úÖ Loaded analysis results from ../data/processed/permits_df.pkl\n",
            "   ‚úÖ building_permits: 8647 rows from cache\n",
            "‚úÖ Loaded analysis results from ../data/processed/cta_df.pkl\n",
            "   ‚úÖ cta_boardings: 668 rows from cache\n",
            "\n",
            "‚úÖ All data loaded from cache\n",
            "\n",
            "==================================================\n",
            "üìä DATASET SUMMARY REPORT\n",
            "==================================================\n",
            "\n",
            "üìà BUSINESS LICENSES\n",
            "------------------------------\n",
            "   Rows: 2,040\n",
            "   Columns: 39\n",
            "   Schema coverage: 100.0%\n",
            "   Date range: 2020-12-03 to 2025-08-29 (1730 days)\n",
            "   Memory usage: 3.1 MB\n",
            "\n",
            "üìà BUILDING PERMITS\n",
            "------------------------------\n",
            "   Rows: 8,647\n",
            "   Columns: 31\n",
            "   Schema coverage: 100.0%\n",
            "   Date range: 2015-03-12 to 2025-08-31 (3825 days)\n",
            "   Memory usage: 9.5 MB\n",
            "\n",
            "üìà CTA BOARDINGS\n",
            "------------------------------\n",
            "   Rows: 668\n",
            "   Columns: 2\n",
            "   Schema coverage: 100.0%\n",
            "   Date range: 2023-09-02 to 2025-06-30 (667 days)\n",
            "   Memory usage: 0.0 MB\n",
            "\n",
            "üéØ TOTAL RECORDS: 11,355\n",
            "üéØ TOTAL MEMORY: 12.6 MB\n"
          ]
        }
      ],
      "source": [
        "# Schema-driven data loading and validation\n",
        "def load_dataset_with_schema_validation(dataset_name: str, sh, worksheet_name: str):\n",
        "    \"\"\"Load dataset and validate against schema.\"\"\"\n",
        "    print(f\"\\nüìä Loading {dataset_name}...\")\n",
        "\n",
        "    # Get schema information\n",
        "    schema = SchemaManager.get_schema(dataset_name)\n",
        "    expected_fields = SchemaManager.get_field_names(dataset_name)\n",
        "    date_fields = SchemaManager.get_date_fields(dataset_name)\n",
        "    required_fields = SchemaManager.get_required_fields(dataset_name)\n",
        "\n",
        "    print(f\"   Schema: {len(expected_fields)} expected fields\")\n",
        "    print(f\"   Date fields: {date_fields}\")\n",
        "    print(f\"   Required fields: {len(required_fields)} required\")\n",
        "\n",
        "    # Load data from sheets\n",
        "    df = load_sheet_data(sh, worksheet_name, parse_dates=date_fields)\n",
        "\n",
        "    if df.empty:\n",
        "        print(f\"   ‚ö†Ô∏è  WARNING: {dataset_name} dataset is empty!\")\n",
        "        return df\n",
        "\n",
        "    # Validate schema compliance\n",
        "    actual_fields = list(df.columns)\n",
        "    missing_expected = set(expected_fields) - set(actual_fields)\n",
        "    extra_fields = set(actual_fields) - set(expected_fields)\n",
        "    missing_required = set(required_fields) - set(actual_fields)\n",
        "\n",
        "    print(f\"   üìà Loaded: {len(df)} rows, {len(actual_fields)} columns\")\n",
        "\n",
        "    # Report schema validation results\n",
        "    if not missing_expected and not extra_fields:\n",
        "        print(f\"   ‚úÖ Schema validation: PERFECT MATCH\")\n",
        "    else:\n",
        "        if missing_expected:\n",
        "            print(f\"   ‚ö†Ô∏è  Missing expected fields ({len(missing_expected)}): {list(missing_expected)[:5]}{'...' if len(missing_expected) > 5 else ''}\")\n",
        "        if extra_fields:\n",
        "            print(f\"   ‚ÑπÔ∏è  Extra fields not in schema ({len(extra_fields)}): {list(extra_fields)[:5]}{'...' if len(extra_fields) > 5 else ''}\")\n",
        "\n",
        "    if missing_required:\n",
        "        print(f\"   ‚ùå CRITICAL: Missing required fields: {missing_required}\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ All required fields present\")\n",
        "\n",
        "    # Validate data quality for required fields\n",
        "    for field in required_fields:\n",
        "        if field in df.columns:\n",
        "            null_count = df[field].isnull().sum()\n",
        "            if null_count > 0:\n",
        "                print(f\"   ‚ö†Ô∏è  Field '{field}' has {null_count} null values ({null_count/len(df)*100:.1f}%)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Define dataset configurations (schema-driven)\n",
        "datasets_config = {\n",
        "    'business_licenses': {\n",
        "        'worksheet': 'Business_Licenses_Full',\n",
        "        'pickle_name': 'licenses_df'\n",
        "    },\n",
        "    'building_permits': {\n",
        "        'worksheet': 'Building_Permits_Full',\n",
        "        'pickle_name': 'permits_df'\n",
        "    },\n",
        "    'cta_boardings': {\n",
        "        'worksheet': 'CTA_Full',\n",
        "        'pickle_name': 'cta_df'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Try to load from saved pickle files first (faster)\n",
        "datasets = {}\n",
        "load_from_sheets = False\n",
        "\n",
        "print(\"üîç Checking for cached data...\")\n",
        "for dataset_name, config in datasets_config.items():\n",
        "    try:\n",
        "        df = load_analysis_results(config['pickle_name'])\n",
        "        if df.empty:\n",
        "            raise FileNotFoundError(f\"{config['pickle_name']} is empty\")\n",
        "        datasets[dataset_name] = df\n",
        "        print(f\"   ‚úÖ {dataset_name}: {len(df)} rows from cache\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"   ‚ùå {dataset_name}: Cache miss\")\n",
        "        load_from_sheets = True\n",
        "\n",
        "if load_from_sheets:\n",
        "    print(\"\\nüìä Loading fresh data from Google Sheets...\")\n",
        "\n",
        "    # Load configuration and connect to Google Sheets\n",
        "    settings = load_settings()\n",
        "    sh = open_sheet(settings.sheet_id, settings.google_creds_path)\n",
        "\n",
        "    # Load and validate each dataset\n",
        "    for dataset_name, config in datasets_config.items():\n",
        "        df = load_dataset_with_schema_validation(\n",
        "            dataset_name,\n",
        "            sh,\n",
        "            config['worksheet']\n",
        "        )\n",
        "        datasets[dataset_name] = df\n",
        "\n",
        "        # Save for future use\n",
        "        save_analysis_results(df, config['pickle_name'])\n",
        "\n",
        "    print(\"\\n‚úÖ All data loaded and cached for future use\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All data loaded from cache\")\n",
        "\n",
        "# Extract datasets for easier access\n",
        "licenses_df = datasets['business_licenses']\n",
        "permits_df = datasets['building_permits']\n",
        "cta_df = datasets['cta_boardings']\n",
        "\n",
        "# Display comprehensive data summaries\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä DATASET SUMMARY REPORT\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for dataset_name, df in datasets.items():\n",
        "    print(f\"\\nüìà {dataset_name.upper().replace('_', ' ')}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"   ‚ö†Ô∏è  Dataset is empty\")\n",
        "        continue\n",
        "\n",
        "    # Basic stats\n",
        "    print(f\"   Rows: {len(df):,}\")\n",
        "    print(f\"   Columns: {len(df.columns)}\")\n",
        "\n",
        "    # Schema compliance\n",
        "    schema = SchemaManager.get_schema(dataset_name)\n",
        "    expected_fields = SchemaManager.get_field_names(dataset_name)\n",
        "    coverage = len(set(df.columns) & set(expected_fields)) / len(expected_fields) * 100\n",
        "    print(f\"   Schema coverage: {coverage:.1f}%\")\n",
        "\n",
        "    # Data freshness (for datasets with date fields)\n",
        "    date_fields = SchemaManager.get_date_fields(dataset_name)\n",
        "    if date_fields and date_fields[0] in df.columns:\n",
        "        main_date_field = date_fields[0]\n",
        "        if not df[main_date_field].empty:\n",
        "            latest_date = pd.to_datetime(df[main_date_field]).max()\n",
        "            oldest_date = pd.to_datetime(df[main_date_field]).min()\n",
        "            days_span = (latest_date - oldest_date).days\n",
        "            print(f\"   Date range: {oldest_date.strftime('%Y-%m-%d')} to {latest_date.strftime('%Y-%m-%d')} ({days_span} days)\")\n",
        "\n",
        "    # Memory usage\n",
        "    memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
        "    print(f\"   Memory usage: {memory_mb:.1f} MB\")\n",
        "\n",
        "print(f\"\\nüéØ TOTAL RECORDS: {sum(len(df) for df in datasets.values()):,}\")\n",
        "print(f\"üéØ TOTAL MEMORY: {sum(df.memory_usage(deep=True).sum() for df in datasets.values()) / 1024 / 1024:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç DETAILED FIELD ANALYSIS: BUSINESS LICENSES\n",
            "============================================================\n",
            "üìä Schema-defined field categories:\n",
            "   ‚Ä¢ Total expected: 39\n",
            "   ‚Ä¢ Date fields: 9\n",
            "   ‚Ä¢ Required fields: 12\n",
            "   ‚Ä¢ Business fields: 11\n",
            "   ‚Ä¢ Geographic fields: 8\n",
            "\n",
            "üìà Field Coverage Analysis:\n",
            "   ‚Ä¢ Present: 39/39 (100.0%)\n",
            "   ‚Ä¢ Missing: 0\n",
            "   ‚Ä¢ Extra: 0\n",
            "\n",
            "üéØ Data Quality by Field Category:\n",
            "\n",
            "   üìã Required Fields (12):\n",
            "      ‚úÖ address: 0 nulls (0.0%), 1678 unique values\n",
            "      ‚úÖ application_type: 0 nulls (0.0%), 1 unique values\n",
            "      ‚úÖ city: 0 nulls (0.0%), 167 unique values\n",
            "      ‚úÖ community_area: 0 nulls (0.0%), 76 unique values\n",
            "      ‚úÖ community_area_name: 0 nulls (0.0%), 76 unique values\n",
            "      ‚úÖ id: 0 nulls (0.0%), 2040 unique values\n",
            "      ‚úÖ legal_name: 0 nulls (0.0%), 1722 unique values\n",
            "      ‚úÖ license_description: 0 nulls (0.0%), 37 unique values\n",
            "      ‚úÖ license_id: 0 nulls (0.0%), 2040 unique values\n",
            "      ‚úÖ license_start_date: 0 nulls (0.0%), 68 unique values\n",
            "      ‚úÖ license_status: 0 nulls (0.0%), 2 unique values\n",
            "      ‚úÖ state: 0 nulls (0.0%), 24 unique values\n",
            "\n",
            "   üìÖ Date Fields (9):\n",
            "      ‚úÖ application_created_date: 2020-12-03 to 2025-08-29 (2 nulls)\n",
            "      ‚úÖ application_requirements_complete: 2020-12-03 to 2025-08-29 (1 nulls)\n",
            "      ‚ö†Ô∏è  conditional_approval: All values are null/invalid\n",
            "      ‚úÖ date_issued: 2025-04-17 to 2025-08-30 (0 nulls)\n",
            "      ‚úÖ expiration_date: 2021-07-20 to 2027-11-15 (2 nulls)\n",
            "      ‚úÖ license_approved_for_issuance: 2021-01-25 to 2025-08-29 (269 nulls)\n",
            "      ‚úÖ license_start_date: 2025-06-03 to 2025-08-30 (0 nulls)\n",
            "      ‚úÖ license_status_change_date: 2025-06-03 to 2025-08-29 (2031 nulls)\n",
            "      ‚úÖ payment_date: 2021-01-25 to 2025-08-29 (11 nulls)\n",
            "\n",
            "   üåç Geographic Fields (8):\n",
            "      ‚úÖ address: 1678 unique values (0.0% nulls)\n",
            "      ‚úÖ community_area_name: 76 unique values (0.0% nulls)\n",
            "      ‚úÖ latitude: Range 41.6530 to 42.0221 (1763 valid, 0 nulls)\n",
            "      ‚úÖ location_human_address: 2 unique values (0.0% nulls)\n",
            "      ‚úÖ location_latitude: 1436 unique values (0.0% nulls)\n",
            "      ‚úÖ location_longitude: 1436 unique values (0.0% nulls)\n",
            "      ‚úÖ longitude: Range -87.9069 to -87.5261 (1763 valid, 0 nulls)\n",
            "      ‚úÖ ward_precinct: 847 unique values (0.0% nulls)\n",
            "\n",
            "üîç DETAILED FIELD ANALYSIS: BUILDING PERMITS\n",
            "============================================================\n",
            "üìä Schema-defined field categories:\n",
            "   ‚Ä¢ Total expected: 31\n",
            "   ‚Ä¢ Date fields: 2\n",
            "   ‚Ä¢ Required fields: 5\n",
            "   ‚Ä¢ Business fields: 5\n",
            "   ‚Ä¢ Geographic fields: 0\n",
            "\n",
            "üìà Field Coverage Analysis:\n",
            "   ‚Ä¢ Present: 31/31 (100.0%)\n",
            "   ‚Ä¢ Missing: 0\n",
            "   ‚Ä¢ Extra: 0\n",
            "\n",
            "üéØ Data Quality by Field Category:\n",
            "\n",
            "   üìã Required Fields (5):\n",
            "      ‚úÖ id: 0 nulls (0.0%), 8647 unique values\n",
            "      ‚úÖ issue_date: 0 nulls (0.0%), 90 unique values\n",
            "      ‚úÖ permit_: 0 nulls (0.0%), 8647 unique values\n",
            "      ‚úÖ permit_status: 0 nulls (0.0%), 4 unique values\n",
            "      ‚úÖ permit_type: 0 nulls (0.0%), 7 unique values\n",
            "\n",
            "   üìÖ Date Fields (2):\n",
            "      ‚úÖ application_start_date: 2015-03-12 to 2025-08-31 (1 nulls)\n",
            "      ‚úÖ issue_date: 2025-06-03 to 2025-08-31 (0 nulls)\n",
            "\n",
            "üîç DETAILED FIELD ANALYSIS: CTA BOARDINGS\n",
            "============================================================\n",
            "üìä Schema-defined field categories:\n",
            "   ‚Ä¢ Total expected: 2\n",
            "   ‚Ä¢ Date fields: 1\n",
            "   ‚Ä¢ Required fields: 2\n",
            "   ‚Ä¢ Business fields: 0\n",
            "   ‚Ä¢ Geographic fields: 0\n",
            "\n",
            "üìà Field Coverage Analysis:\n",
            "   ‚Ä¢ Present: 2/2 (100.0%)\n",
            "   ‚Ä¢ Missing: 0\n",
            "   ‚Ä¢ Extra: 0\n",
            "\n",
            "üéØ Data Quality by Field Category:\n",
            "\n",
            "   üìã Required Fields (2):\n",
            "      ‚úÖ service_date: 0 nulls (0.0%), 668 unique values\n",
            "      ‚úÖ total_rides: 0 nulls (0.0%), 668 unique values\n",
            "\n",
            "   üìÖ Date Fields (1):\n",
            "      ‚úÖ service_date: 2023-09-02 to 2025-06-30 (0 nulls)\n"
          ]
        }
      ],
      "source": [
        "# Detailed field analysis and data validation\n",
        "def analyze_dataset_fields(dataset_name: str, df: pd.DataFrame):\n",
        "    \"\"\"Perform detailed field analysis for a dataset.\"\"\"\n",
        "    print(f\"\\nüîç DETAILED FIELD ANALYSIS: {dataset_name.upper().replace('_', ' ')}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"   ‚ö†Ô∏è  Dataset is empty - skipping analysis\")\n",
        "        return\n",
        "\n",
        "    # Get schema information\n",
        "    schema = SchemaManager.get_schema(dataset_name)\n",
        "    expected_fields = SchemaManager.get_field_names(dataset_name)\n",
        "    date_fields = SchemaManager.get_date_fields(dataset_name)\n",
        "    required_fields = SchemaManager.get_required_fields(dataset_name)\n",
        "    business_fields = SchemaManager.get_business_fields(dataset_name)\n",
        "    geographic_fields = SchemaManager.get_geographic_fields(dataset_name)\n",
        "\n",
        "    print(f\"üìä Schema-defined field categories:\")\n",
        "    print(f\"   ‚Ä¢ Total expected: {len(expected_fields)}\")\n",
        "    print(f\"   ‚Ä¢ Date fields: {len(date_fields)}\")\n",
        "    print(f\"   ‚Ä¢ Required fields: {len(required_fields)}\")\n",
        "    print(f\"   ‚Ä¢ Business fields: {len(business_fields)}\")\n",
        "    print(f\"   ‚Ä¢ Geographic fields: {len(geographic_fields)}\")\n",
        "\n",
        "    # Analyze field compliance\n",
        "    actual_fields = set(df.columns)\n",
        "    expected_set = set(expected_fields)\n",
        "\n",
        "    present_fields = actual_fields & expected_set\n",
        "    missing_fields = expected_set - actual_fields\n",
        "    extra_fields = actual_fields - expected_set\n",
        "\n",
        "    print(f\"\\nüìà Field Coverage Analysis:\")\n",
        "    print(f\"   ‚Ä¢ Present: {len(present_fields)}/{len(expected_fields)} ({len(present_fields)/len(expected_fields)*100:.1f}%)\")\n",
        "    print(f\"   ‚Ä¢ Missing: {len(missing_fields)}\")\n",
        "    print(f\"   ‚Ä¢ Extra: {len(extra_fields)}\")\n",
        "\n",
        "    if missing_fields:\n",
        "        print(f\"\\n‚ùå Missing fields: {sorted(list(missing_fields))}\")\n",
        "\n",
        "    if extra_fields:\n",
        "        print(f\"\\n‚ûï Extra fields (not in schema): {sorted(list(extra_fields))[:10]}{'...' if len(extra_fields) > 10 else ''}\")\n",
        "\n",
        "    # Analyze data quality by field category\n",
        "    print(f\"\\nüéØ Data Quality by Field Category:\")\n",
        "\n",
        "    # Required fields analysis\n",
        "    print(f\"\\n   üìã Required Fields ({len(required_fields)}):\")\n",
        "    for field in sorted(required_fields):\n",
        "        if field in df.columns:\n",
        "            null_count = df[field].isnull().sum()\n",
        "            null_pct = null_count / len(df) * 100\n",
        "            unique_count = df[field].nunique()\n",
        "            print(f\"      ‚úÖ {field}: {null_count} nulls ({null_pct:.1f}%), {unique_count} unique values\")\n",
        "        else:\n",
        "            print(f\"      ‚ùå {field}: MISSING FROM DATASET\")\n",
        "\n",
        "    # Date fields analysis\n",
        "    if date_fields:\n",
        "        print(f\"\\n   üìÖ Date Fields ({len(date_fields)}):\")\n",
        "        for field in sorted(date_fields):\n",
        "            if field in df.columns:\n",
        "                try:\n",
        "                    date_series = pd.to_datetime(df[field], errors='coerce')\n",
        "                    null_count = date_series.isnull().sum()\n",
        "                    if not date_series.dropna().empty:\n",
        "                        min_date = date_series.min()\n",
        "                        max_date = date_series.max()\n",
        "                        print(f\"      ‚úÖ {field}: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')} ({null_count} nulls)\")\n",
        "                    else:\n",
        "                        print(f\"      ‚ö†Ô∏è  {field}: All values are null/invalid\")\n",
        "                except Exception as e:\n",
        "                    print(f\"      ‚ùå {field}: Error parsing dates - {str(e)[:50]}\")\n",
        "            else:\n",
        "                print(f\"      ‚ùå {field}: MISSING FROM DATASET\")\n",
        "\n",
        "    # Geographic fields analysis\n",
        "    if geographic_fields:\n",
        "        print(f\"\\n   üåç Geographic Fields ({len(geographic_fields)}):\")\n",
        "        for field in sorted(geographic_fields):\n",
        "            if field in df.columns:\n",
        "                null_count = df[field].isnull().sum()\n",
        "                null_pct = null_count / len(df) * 100\n",
        "                if field in ['latitude', 'longitude']:\n",
        "                    try:\n",
        "                        numeric_vals = pd.to_numeric(df[field], errors='coerce')\n",
        "                        valid_count = numeric_vals.notna().sum()\n",
        "                        if valid_count > 0:\n",
        "                            min_val = numeric_vals.min()\n",
        "                            max_val = numeric_vals.max()\n",
        "                            print(f\"      ‚úÖ {field}: Range {min_val:.4f} to {max_val:.4f} ({valid_count} valid, {null_count} nulls)\")\n",
        "                        else:\n",
        "                            print(f\"      ‚ö†Ô∏è  {field}: No valid numeric values\")\n",
        "                    except:\n",
        "                        print(f\"      ‚ùå {field}: Error parsing numeric values\")\n",
        "                else:\n",
        "                    unique_count = df[field].nunique()\n",
        "                    print(f\"      ‚úÖ {field}: {unique_count} unique values ({null_pct:.1f}% nulls)\")\n",
        "            else:\n",
        "                print(f\"      ‚ùå {field}: MISSING FROM DATASET\")\n",
        "\n",
        "# Run detailed analysis for each dataset\n",
        "for dataset_name, df in datasets.items():\n",
        "    analyze_dataset_fields(dataset_name, df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üîç DATA PIPELINE VALIDATION REPORT\n",
            "================================================================================\n",
            "\n",
            "üìä VALIDATION SUMMARY TABLE:\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Dataset        Status      Rows      Cols Schema  RequiredDate Range            Issues                                  \n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Business Li... ‚úÖ EXCELLENT 2,040     39   100.0%  100.0%  2020-12-03 to 2025... None                                    \n",
            "Building Pe... ‚úÖ EXCELLENT 8,647     31   100.0%  100.0%  2015-03-12 to 2025... None                                    \n",
            "Cta Boardings  ‚úÖ EXCELLENT 668       2    100.0%  100.0%  2023-09-02 to 2025... None                                    \n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "üìà PIPELINE SUMMARY:\n",
            "   ‚Ä¢ Total records processed: 11,355\n",
            "   ‚Ä¢ Datasets with excellent quality: 3/3\n",
            "   ‚Ä¢ Datasets with good+ quality: 3/3\n",
            "\n",
            "üéØ SCHEMA COMPLIANCE:\n",
            "   ‚Ä¢ business_licenses: 39/39 fields\n",
            "   ‚Ä¢ building_permits: 31/31 fields\n",
            "   ‚Ä¢ cta_boardings: 2/2 fields\n"
          ]
        }
      ],
      "source": [
        "# Create validation summary table to verify data pipeline integrity\n",
        "def create_validation_report():\n",
        "    \"\"\"Create a comprehensive validation report.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üîç DATA PIPELINE VALIDATION REPORT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    validation_data = []\n",
        "\n",
        "    for dataset_name, df in datasets.items():\n",
        "        # Get schema info\n",
        "        schema = SchemaManager.get_schema(dataset_name)\n",
        "        expected_fields = SchemaManager.get_field_names(dataset_name)\n",
        "        date_fields = SchemaManager.get_date_fields(dataset_name)\n",
        "        required_fields = SchemaManager.get_required_fields(dataset_name)\n",
        "\n",
        "        if df.empty:\n",
        "            validation_data.append({\n",
        "                'Dataset': dataset_name,\n",
        "                'Status': '‚ùå EMPTY',\n",
        "                'Rows': 0,\n",
        "                'Columns': 0,\n",
        "                'Schema Match': '0%',\n",
        "                'Required Fields': '0%',\n",
        "                'Date Range': 'N/A',\n",
        "                'Issues': 'Dataset is empty'\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Calculate metrics\n",
        "        actual_fields = set(df.columns)\n",
        "        expected_set = set(expected_fields)\n",
        "        present_fields = actual_fields & expected_set\n",
        "        schema_match_pct = len(present_fields) / len(expected_fields) * 100\n",
        "\n",
        "        required_present = set(required_fields) & actual_fields\n",
        "        required_match_pct = len(required_present) / len(required_fields) * 100 if required_fields else 100\n",
        "\n",
        "        # Check date range\n",
        "        date_range = \"N/A\"\n",
        "        if date_fields and date_fields[0] in df.columns:\n",
        "            try:\n",
        "                dates = pd.to_datetime(df[date_fields[0]], errors='coerce').dropna()\n",
        "                if not dates.empty:\n",
        "                    date_range = f\"{dates.min().strftime('%Y-%m-%d')} to {dates.max().strftime('%Y-%m-%d')}\"\n",
        "            except:\n",
        "                date_range = \"Invalid dates\"\n",
        "\n",
        "        # Identify issues\n",
        "        issues = []\n",
        "        if schema_match_pct < 100:\n",
        "            issues.append(f\"{len(expected_set - actual_fields)} missing fields\")\n",
        "        if required_match_pct < 100:\n",
        "            issues.append(f\"{len(set(required_fields) - actual_fields)} missing required\")\n",
        "\n",
        "        # Check for null values in required fields\n",
        "        null_issues = []\n",
        "        for field in required_fields:\n",
        "            if field in df.columns:\n",
        "                null_count = df[field].isnull().sum()\n",
        "                if null_count > 0:\n",
        "                    null_issues.append(f\"{field}({null_count})\")\n",
        "        if null_issues:\n",
        "            issues.append(f\"Nulls in required: {', '.join(null_issues[:3])}\")\n",
        "\n",
        "        # Overall status\n",
        "        if schema_match_pct == 100 and required_match_pct == 100 and not null_issues:\n",
        "            status = \"‚úÖ EXCELLENT\"\n",
        "        elif schema_match_pct >= 90 and required_match_pct == 100:\n",
        "            status = \"‚úÖ GOOD\"\n",
        "        elif schema_match_pct >= 70 and required_match_pct >= 90:\n",
        "            status = \"‚ö†Ô∏è FAIR\"\n",
        "        else:\n",
        "            status = \"‚ùå POOR\"\n",
        "\n",
        "        validation_data.append({\n",
        "            'Dataset': dataset_name.replace('_', ' ').title(),\n",
        "            'Status': status,\n",
        "            'Rows': f\"{len(df):,}\",\n",
        "            'Columns': len(df.columns),\n",
        "            'Schema Match': f\"{schema_match_pct:.1f}%\",\n",
        "            'Required Fields': f\"{required_match_pct:.1f}%\",\n",
        "            'Date Range': date_range,\n",
        "            'Issues': '; '.join(issues) if issues else 'None'\n",
        "        })\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    validation_df = pd.DataFrame(validation_data)\n",
        "\n",
        "    # Display formatted table\n",
        "    print(\"\\nüìä VALIDATION SUMMARY TABLE:\")\n",
        "    print(\"-\" * 120)\n",
        "\n",
        "    # Print headers\n",
        "    headers = ['Dataset', 'Status', 'Rows', 'Cols', 'Schema', 'Required', 'Date Range', 'Issues']\n",
        "    col_widths = [15, 12, 10, 5, 8, 8, 22, 40]\n",
        "\n",
        "    header_row = \"\"\n",
        "    for header, width in zip(headers, col_widths):\n",
        "        header_row += f\"{header:<{width}}\"\n",
        "    print(header_row)\n",
        "    print(\"-\" * 120)\n",
        "\n",
        "    # Print data rows\n",
        "    for _, row in validation_df.iterrows():\n",
        "        data_row = \"\"\n",
        "        for (col, width) in zip(['Dataset', 'Status', 'Rows', 'Columns', 'Schema Match', 'Required Fields', 'Date Range', 'Issues'], col_widths):\n",
        "            value = str(row[col])\n",
        "            if len(value) > width-1:\n",
        "                value = value[:width-4] + \"...\"\n",
        "            data_row += f\"{value:<{width}}\"\n",
        "        print(data_row)\n",
        "\n",
        "    print(\"-\" * 120)\n",
        "\n",
        "    # Summary statistics\n",
        "    total_rows = sum(int(row['Rows'].replace(',', '')) for _, row in validation_df.iterrows() if row['Rows'] != '0')\n",
        "    excellent_count = sum(1 for _, row in validation_df.iterrows() if row['Status'].startswith('‚úÖ EXCELLENT'))\n",
        "    good_count = sum(1 for _, row in validation_df.iterrows() if row['Status'].startswith('‚úÖ GOOD'))\n",
        "\n",
        "    print(f\"\\nüìà PIPELINE SUMMARY:\")\n",
        "    print(f\"   ‚Ä¢ Total records processed: {total_rows:,}\")\n",
        "    print(f\"   ‚Ä¢ Datasets with excellent quality: {excellent_count}/{len(validation_df)}\")\n",
        "    print(f\"   ‚Ä¢ Datasets with good+ quality: {excellent_count + good_count}/{len(validation_df)}\")\n",
        "\n",
        "    # Schema compliance report\n",
        "    print(f\"\\nüéØ SCHEMA COMPLIANCE:\")\n",
        "    for dataset_name in datasets.keys():\n",
        "        schema = SchemaManager.get_schema(dataset_name)\n",
        "        expected_count = len(SchemaManager.get_field_names(dataset_name))\n",
        "        actual_count = len(datasets[dataset_name].columns) if not datasets[dataset_name].empty else 0\n",
        "        print(f\"   ‚Ä¢ {dataset_name}: {actual_count}/{expected_count} fields\")\n",
        "\n",
        "    return validation_df\n",
        "\n",
        "# Generate the validation report\n",
        "validation_summary = create_validation_report()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Data Quality Heatmap & Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ENHANCED DATA QUALITY MATRIX\n",
            "============================================================\n",
            "\n",
            "BUSINESS LICENSES\n",
            "----------------------------------------\n",
            "   EXCELLENT  id                        1.000 CRITICAL\n",
            "   EXCELLENT  license_id                1.000 CRITICAL\n",
            "   EXCELLENT  account_number            1.000 OPTIONAL\n",
            "   EXCELLENT  site_number               1.000 OPTIONAL\n",
            "   EXCELLENT  legal_name                0.998 CRITICAL\n",
            "   EXCELLENT  doing_business_as_name    0.999 OPTIONAL\n",
            "   EXCELLENT  license_code              1.000 OPTIONAL\n",
            "   EXCELLENT  license_number            1.000 OPTIONAL\n",
            "   EXCELLENT  license_description       0.990 CRITICAL\n",
            "   EXCELLENT  business_activity_id      0.962 OPTIONAL [CONTAMINATED]\n",
            "      -> Length variation: std=5.4\n",
            "   EXCELLENT  business_activity         0.992 OPTIONAL\n",
            "   EXCELLENT  address                   0.998 CRITICAL\n",
            "   EXCELLENT  city                      0.991 CRITICAL\n",
            "   EXCELLENT  state                     0.990 CRITICAL\n",
            "   EXCELLENT  zip_code                  0.991 OPTIONAL\n",
            "      -> Non-numeric contamination: 4 records (0.2%)\n",
            "      -> Examples: ['M6K', '', '']\n",
            "   GOOD       ward                      0.949 OPTIONAL [CONTAMINATED]\n",
            "      -> Non-numeric contamination: 280 records (13.7%)\n",
            "      -> Examples: ['', '', '']\n",
            "   EXCELLENT  precinct                  0.952 OPTIONAL [CONTAMINATED]\n",
            "      -> Non-numeric contamination: 261 records (12.8%)\n",
            "      -> Examples: ['', '', '']\n",
            "   EXCELLENT  ward_precinct             0.994 OPTIONAL\n",
            "   EXCELLENT  police_district           0.990 OPTIONAL\n",
            "   EXCELLENT  community_area            0.954 CRITICAL [CONTAMINATED]\n",
            "      -> Non-numeric contamination: 244 records (12.0%)\n",
            "      -> Examples: ['', '', '']\n",
            "   EXCELLENT  community_area_name       0.990 CRITICAL\n",
            "   EXCELLENT  neighborhood              0.990 OPTIONAL\n",
            "   EXCELLENT  ssa                       0.990 OPTIONAL\n",
            "   EXCELLENT  latitude                  0.956 OPTIONAL [CONTAMINATED]\n",
            "      -> Non-numeric contamination: 277 records (13.6%)\n",
            "      -> Examples: ['', '', '']\n",
            "   EXCELLENT  longitude                 0.956 OPTIONAL [CONTAMINATED]\n",
            "      -> Non-numeric contamination: 277 records (13.6%)\n",
            "      -> Examples: ['', '', '']\n",
            "   EXCELLENT  location_latitude         0.997 OPTIONAL\n",
            "   EXCELLENT  location_longitude        0.997 OPTIONAL\n",
            "   EXCELLENT  location_human_address    0.990 OPTIONAL\n",
            "   EXCELLENT  application_type          0.990 CRITICAL\n",
            "   EXCELLENT  application_created_date  1.000 OPTIONAL\n",
            "   EXCELLENT  application_requirements_complete 1.000 OPTIONAL\n",
            "   EXCELLENT  payment_date              0.999 OPTIONAL\n",
            "   NULL FIELD conditional_approval      0.000 (ALL NULL)\n",
            "   EXCELLENT  license_approved_for_issuance 0.967 OPTIONAL\n",
            "   EXCELLENT  date_issued               1.000 OPTIONAL\n",
            "   EXCELLENT  license_start_date        1.000 CRITICAL\n",
            "   EXCELLENT  expiration_date           1.000 OPTIONAL\n",
            "   EXCELLENT  license_status            0.990 CRITICAL\n",
            "   FAIR       license_status_change_date 0.751 OPTIONAL\n",
            "\n",
            "BUILDING PERMITS\n",
            "----------------------------------------\n",
            "   EXCELLENT  id                        1.000 CRITICAL\n",
            "   EXCELLENT  permit_                   1.000 CRITICAL\n",
            "   EXCELLENT  permit_status             0.990 CRITICAL\n",
            "   EXCELLENT  permit_milestone          0.990 OPTIONAL\n",
            "   GOOD       permit_type               0.943 CRITICAL [CONTAMINATED]\n",
            "      -> Encoding issues: 5407 records\n",
            "   EXCELLENT  review_type               0.990 OPTIONAL\n",
            "   EXCELLENT  application_start_date    1.000 OPTIONAL\n",
            "   EXCELLENT  issue_date                1.000 CRITICAL\n",
            "   EXCELLENT  processing_time           0.990 OPTIONAL\n",
            "   EXCELLENT  street_number             0.994 OPTIONAL\n",
            "   EXCELLENT  street_direction          0.990 OPTIONAL\n",
            "   EXCELLENT  street_name               0.991 OPTIONAL\n",
            "   EXCELLENT  community_area            0.988 OPTIONAL\n",
            "      -> Non-numeric contamination: 66 records (0.8%)\n",
            "      -> Examples: ['', '', '']\n",
            "   EXCELLENT  work_type                 0.990 OPTIONAL\n",
            "   EXCELLENT  work_description          0.997 OPTIONAL\n",
            "      -> Leading special chars: 3 records\n",
            "      -> Trailing special chars: 53 records\n",
            "   EXCELLENT  building_fee_paid         1.000 OPTIONAL\n",
            "   EXCELLENT  zoning_fee_paid           1.000 OPTIONAL\n",
            "   EXCELLENT  other_fee_paid            1.000 OPTIONAL\n",
            "   EXCELLENT  subtotal_paid             1.000 OPTIONAL\n",
            "   EXCELLENT  building_fee_unpaid       1.000 OPTIONAL\n",
            "   EXCELLENT  zoning_fee_unpaid         1.000 OPTIONAL\n",
            "   EXCELLENT  other_fee_unpaid          1.000 OPTIONAL\n",
            "   EXCELLENT  subtotal_unpaid           1.000 OPTIONAL\n",
            "   EXCELLENT  building_fee_waived       1.000 OPTIONAL\n",
            "   EXCELLENT  building_fee_subtotal     1.000 OPTIONAL\n",
            "   EXCELLENT  zoning_fee_subtotal       1.000 OPTIONAL\n",
            "   EXCELLENT  other_fee_subtotal        1.000 OPTIONAL\n",
            "   EXCELLENT  zoning_fee_waived         1.000 OPTIONAL\n",
            "   EXCELLENT  other_fee_waived          1.000 OPTIONAL\n",
            "   EXCELLENT  subtotal_waived           1.000 OPTIONAL\n",
            "   EXCELLENT  total_fee                 1.000 OPTIONAL\n",
            "\n",
            "CTA BOARDINGS\n",
            "----------------------------------------\n",
            "   EXCELLENT  service_date              1.000 CRITICAL\n",
            "   EXCELLENT  total_rides               1.000 CRITICAL\n",
            "\n",
            "ENHANCED QUALITY SUMMARY\n",
            "-----------------------------------\n",
            "   Overall Quality Score: 0.988\n",
            "   Data Purity Score: 0.987\n",
            "   Critical Issues: 0\n",
            "   Contaminated Fields: 7\n",
            "   Fields Needing Attention: 1\n",
            "\n",
            "CONTAMINATION DETAILS\n",
            "-------------------------\n",
            "   business_licenses.business_activity_id: 0.900\n",
            "      Length variation: std=5.4\n",
            "   business_licenses.ward: 0.863\n",
            "      Non-numeric contamination: 280 records (13.7%)\n",
            "   business_licenses.precinct: 0.872\n",
            "      Non-numeric contamination: 261 records (12.8%)\n",
            "   business_licenses.community_area: 0.880\n",
            "      Non-numeric contamination: 244 records (12.0%)\n",
            "   business_licenses.latitude: 0.864\n",
            "      Non-numeric contamination: 277 records (13.6%)\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Data Quality Matrix with Mixed Type Detection\n",
        "def create_enhanced_data_quality_matrix():\n",
        "    \"\"\"Create comprehensive data quality assessment matrix with mixed type detection.\"\"\"\n",
        "    print(\"ENHANCED DATA QUALITY MATRIX\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    quality_data = []\n",
        "    contamination_issues = []\n",
        "\n",
        "    for dataset_name, df in datasets.items():\n",
        "        print(f\"\\n{dataset_name.upper().replace('_', ' ')}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"   WARNING: Dataset is empty\")\n",
        "            continue\n",
        "\n",
        "        schema_fields = SchemaManager.get_field_names(dataset_name)\n",
        "        required_fields = SchemaManager.get_required_fields(dataset_name)\n",
        "        date_fields = SchemaManager.get_date_fields(dataset_name)\n",
        "\n",
        "        for field in schema_fields:\n",
        "            if field in df.columns:\n",
        "                # Get non-null values for analysis\n",
        "                non_null_values = df[field].dropna()\n",
        "\n",
        "                if len(non_null_values) == 0:\n",
        "                    print(f\"   NULL FIELD {field:<25} 0.000 (ALL NULL)\")\n",
        "                    continue\n",
        "\n",
        "                # Convert to string for pattern analysis\n",
        "                str_values = non_null_values.astype(str)\n",
        "\n",
        "                # Basic quality metrics\n",
        "                null_pct = df[field].isnull().sum() / len(df)\n",
        "\n",
        "                # MIXED TYPE DETECTION\n",
        "                contamination_score = 1.0\n",
        "                contamination_details = []\n",
        "\n",
        "                # 1. Detect numeric contamination in supposedly numeric fields\n",
        "                if field in ['community_area', 'ward', 'precinct', 'zip_code', 'latitude', 'longitude']:\n",
        "                    # Check for non-numeric characters in numeric fields\n",
        "                    numeric_pattern = r'^-?\\d+\\.?\\d*$'\n",
        "                    pure_numeric = str_values.str.match(numeric_pattern, na=False)\n",
        "                    contaminated_count = (~pure_numeric).sum()\n",
        "\n",
        "                    if contaminated_count > 0:\n",
        "                        contamination_pct = (contaminated_count / len(str_values)) * 100\n",
        "                        contamination_score *= (1 - contamination_pct / 100)\n",
        "                        contamination_details.append(f\"Non-numeric contamination: {contaminated_count} records ({contamination_pct:.1f}%)\")\n",
        "\n",
        "                        # Show examples of contaminated values\n",
        "                        contaminated_examples = str_values[~pure_numeric].head(3).tolist()\n",
        "                        contamination_details.append(f\"Examples: {contaminated_examples}\")\n",
        "\n",
        "                # 2. Detect leading/trailing special characters\n",
        "                special_char_issues = 0\n",
        "                if df[field].dtype == 'object':\n",
        "                    # Check for leading quotes, spaces, or other unwanted characters\n",
        "                    leading_issues = str_values.str.match(r'^[\\'\\\"\\s`]+', na=False).sum()\n",
        "                    trailing_issues = str_values.str.match(r'.*[\\'\\\"\\s`]+$', na=False).sum()\n",
        "\n",
        "                    if leading_issues > 0:\n",
        "                        special_char_issues += leading_issues\n",
        "                        contamination_details.append(f\"Leading special chars: {leading_issues} records\")\n",
        "\n",
        "                    if trailing_issues > 0:\n",
        "                        special_char_issues += trailing_issues\n",
        "                        contamination_details.append(f\"Trailing special chars: {trailing_issues} records\")\n",
        "\n",
        "                    if special_char_issues > 0:\n",
        "                        special_char_pct = (special_char_issues / len(str_values)) * 100\n",
        "                        contamination_score *= (1 - special_char_pct / 200)  # Less penalty than full contamination\n",
        "\n",
        "                # 3. Detect mixed case inconsistency in categorical fields\n",
        "                case_inconsistency = 0\n",
        "                if field in ['city', 'state', 'license_status', 'permit_status', 'license_description']:\n",
        "                    unique_values = str_values.unique()\n",
        "                    unique_upper = str_values.str.upper().unique()\n",
        "\n",
        "                    if len(unique_upper) < len(unique_values):\n",
        "                        case_inconsistency = len(unique_values) - len(unique_upper)\n",
        "                        case_pct = (case_inconsistency / len(unique_values)) * 100\n",
        "                        contamination_score *= (1 - case_pct / 300)  # Smaller penalty\n",
        "                        contamination_details.append(f\"Case inconsistency: {case_inconsistency} variations\")\n",
        "\n",
        "                # 4. Detect encoding issues (non-ASCII characters where not expected)\n",
        "                encoding_issues = 0\n",
        "                if df[field].dtype == 'object':\n",
        "                    try:\n",
        "                        non_ascii = str_values.str.encode('ascii', errors='ignore').str.decode('ascii')\n",
        "                        encoding_issues = (str_values != non_ascii).sum()\n",
        "\n",
        "                        if encoding_issues > 0:\n",
        "                            encoding_pct = (encoding_issues / len(str_values)) * 100\n",
        "                            contamination_score *= (1 - encoding_pct / 400)  # Small penalty\n",
        "                            contamination_details.append(f\"Encoding issues: {encoding_issues} records\")\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                # 5. Detect length inconsistencies in ID fields\n",
        "                length_inconsistency = 0\n",
        "                if 'id' in field.lower() or field in ['license_number', 'permit_']:\n",
        "                    lengths = str_values.str.len()\n",
        "                    length_std = lengths.std()\n",
        "\n",
        "                    if length_std > 2:  # High variation in ID lengths\n",
        "                        length_inconsistency = 1\n",
        "                        contamination_score *= 0.9\n",
        "                        contamination_details.append(f\"Length variation: std={length_std:.1f}\")\n",
        "\n",
        "                # Type consistency check\n",
        "                type_consistency = 1.0\n",
        "                if field in date_fields:\n",
        "                    try:\n",
        "                        parsed_dates = pd.to_datetime(df[field], errors='coerce')\n",
        "                        invalid_dates = parsed_dates.isna().sum() - df[field].isna().sum()\n",
        "                        if invalid_dates > 0:\n",
        "                            type_consistency = 1 - (invalid_dates / len(df))\n",
        "                    except:\n",
        "                        type_consistency = 0.5\n",
        "\n",
        "                # Duplicate assessment\n",
        "                duplicate_factor = 0\n",
        "                if df[field].dtype == 'object' and len(df) > 1:\n",
        "                    duplicate_pct = df[field].duplicated().sum() / len(df)\n",
        "                    duplicate_factor = min(duplicate_pct * 0.1, 0.1)\n",
        "\n",
        "                # Business logic compliance (enhanced)\n",
        "                business_compliance = 1.0\n",
        "                if field in ['latitude', 'longitude'] and dataset_name == 'business_licenses':\n",
        "                    try:\n",
        "                        numeric_vals = pd.to_numeric(df[field], errors='coerce')\n",
        "                        valid_vals = numeric_vals.dropna()\n",
        "\n",
        "                        if len(valid_vals) > 0:\n",
        "                            if field == 'latitude':\n",
        "                                out_of_bounds = ((valid_vals < 41.6) | (valid_vals > 42.1)).sum()\n",
        "                            else:\n",
        "                                out_of_bounds = ((valid_vals < -87.9) | (valid_vals > -87.5)).sum()\n",
        "                            business_compliance = 1 - (out_of_bounds / len(df))\n",
        "                    except:\n",
        "                        business_compliance = 0.5\n",
        "\n",
        "                # Enhanced composite quality score\n",
        "                quality_score = (\n",
        "                    (1 - null_pct) * 0.25 +          # Completeness (25%)\n",
        "                    type_consistency * 0.25 +         # Type consistency (25%)\n",
        "                    contamination_score * 0.30 +      # Contamination/purity (30%)\n",
        "                    (1 - duplicate_factor) * 0.10 +   # Uniqueness (10%)\n",
        "                    business_compliance * 0.10        # Business logic (10%)\n",
        "                )\n",
        "\n",
        "                # Quality assessment with contamination consideration\n",
        "                if quality_score >= 0.95:\n",
        "                    status = \"EXCELLENT\"\n",
        "                elif quality_score >= 0.85:\n",
        "                    status = \"GOOD\"\n",
        "                elif quality_score >= 0.70:\n",
        "                    status = \"FAIR\"\n",
        "                elif quality_score >= 0.50:\n",
        "                    status = \"POOR\"\n",
        "                else:\n",
        "                    status = \"CRITICAL\"\n",
        "\n",
        "                importance = \"CRITICAL\" if field in required_fields else \"OPTIONAL\"\n",
        "\n",
        "                # Display results\n",
        "                contamination_indicator = \"\"\n",
        "                if contamination_score < 0.95:\n",
        "                    contamination_indicator = \" [CONTAMINATED]\"\n",
        "                elif contamination_score < 0.99:\n",
        "                    contamination_indicator = \" [MINOR_ISSUES]\"\n",
        "\n",
        "                print(f\"   {status:<10} {field:<25} {quality_score:.3f} {importance}{contamination_indicator}\")\n",
        "\n",
        "                # Show contamination details\n",
        "                if contamination_details:\n",
        "                    for detail in contamination_details[:2]:  # Show first 2 issues\n",
        "                        print(f\"      -> {detail}\")\n",
        "\n",
        "                # Store contamination issues for summary\n",
        "                if contamination_score < 0.95:\n",
        "                    contamination_issues.append({\n",
        "                        'dataset': dataset_name,\n",
        "                        'field': field,\n",
        "                        'contamination_score': contamination_score,\n",
        "                        'issues': contamination_details\n",
        "                    })\n",
        "\n",
        "                # Add to summary data\n",
        "                quality_data.append({\n",
        "                    'dataset': dataset_name,\n",
        "                    'field': field,\n",
        "                    'quality_score': quality_score,\n",
        "                    'contamination_score': contamination_score,\n",
        "                    'null_pct': null_pct * 100,\n",
        "                    'is_required': field in required_fields,\n",
        "                    'status': status,\n",
        "                    'contamination_issues': len(contamination_details)\n",
        "                })\n",
        "            else:\n",
        "                print(f\"   MISSING    {field:<25} 0.000 CRITICAL\")\n",
        "                quality_data.append({\n",
        "                    'dataset': dataset_name,\n",
        "                    'field': field,\n",
        "                    'quality_score': 0.0,\n",
        "                    'contamination_score': 0.0,\n",
        "                    'null_pct': 100.0,\n",
        "                    'is_required': field in required_fields,\n",
        "                    'status': 'MISSING',\n",
        "                    'contamination_issues': 1\n",
        "                })\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    quality_df = pd.DataFrame(quality_data)\n",
        "\n",
        "    print(f\"\\nENHANCED QUALITY SUMMARY\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    if not quality_df.empty:\n",
        "        overall_score = quality_df['quality_score'].mean()\n",
        "        contamination_avg = quality_df['contamination_score'].mean()\n",
        "        critical_issues = len(quality_df[(quality_df['is_required']) & (quality_df['quality_score'] < 0.8)])\n",
        "        contaminated_fields = len(quality_df[quality_df['contamination_score'] < 0.95])\n",
        "\n",
        "        print(f\"   Overall Quality Score: {overall_score:.3f}\")\n",
        "        print(f\"   Data Purity Score: {contamination_avg:.3f}\")\n",
        "        print(f\"   Critical Issues: {critical_issues}\")\n",
        "        print(f\"   Contaminated Fields: {contaminated_fields}\")\n",
        "        print(f\"   Fields Needing Attention: {len(quality_df[quality_df['quality_score'] < 0.85])}\")\n",
        "\n",
        "    # Contamination summary\n",
        "    if contamination_issues:\n",
        "        print(f\"\\nCONTAMINATION DETAILS\")\n",
        "        print(\"-\" * 25)\n",
        "        for issue in contamination_issues[:5]:  # Show top 5 contamination issues\n",
        "            print(f\"   {issue['dataset']}.{issue['field']}: {issue['contamination_score']:.3f}\")\n",
        "            for detail in issue['issues'][:1]:  # Show first issue\n",
        "                print(f\"      {detail}\")\n",
        "\n",
        "    return quality_df, contamination_issues\n",
        "\n",
        "# Run enhanced quality assessment\n",
        "quality_matrix, contamination_report = create_enhanced_data_quality_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Automated Anamoly Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA CONTAMINATION ANALYSIS\n",
            "=============================================\n",
            "\n",
            "BUSINESS LICENSES CONTAMINATION SCAN\n",
            "--------------------------------------------------\n",
            "\n",
            "   Analyzing id (text field)\n",
            "\n",
            "   Analyzing legal_name (text field)\n",
            "\n",
            "   Analyzing doing_business_as_name (text field)\n",
            "\n",
            "   Analyzing license_description (text field)\n",
            "\n",
            "   Analyzing business_activity_id (text field)\n",
            "\n",
            "   Analyzing business_activity (text field)\n",
            "\n",
            "   Analyzing address (text field)\n",
            "\n",
            "   Analyzing city (text field)\n",
            "\n",
            "   Analyzing state (text field)\n",
            "\n",
            "   Analyzing zip_code (expected: numeric)\n",
            "      LETTERS: 1 records with letters - ['M6K']\n",
            "\n",
            "   Analyzing ward (expected: numeric)\n",
            "      CLEAN: All 2040 values are properly numeric\n",
            "\n",
            "   Analyzing precinct (expected: numeric)\n",
            "      CLEAN: All 2040 values are properly numeric\n",
            "\n",
            "   Analyzing ward_precinct (text field)\n",
            "\n",
            "   Analyzing police_district (text field)\n",
            "\n",
            "   Analyzing community_area (expected: numeric)\n",
            "      CLEAN: All 2040 values are properly numeric\n",
            "\n",
            "   Analyzing community_area_name (text field)\n",
            "\n",
            "   Analyzing neighborhood (text field)\n",
            "\n",
            "   Analyzing latitude (text field)\n",
            "\n",
            "   Analyzing longitude (text field)\n",
            "\n",
            "   Analyzing application_type (text field)\n",
            "\n",
            "   Analyzing application_created_date (date field)\n",
            "      CLEAN: Consistent iso_date format\n",
            "\n",
            "   Analyzing application_requirements_complete (date field)\n",
            "      CLEAN: Consistent iso_date format\n",
            "\n",
            "   Analyzing payment_date (date field)\n",
            "      CLEAN: Consistent iso_date format\n",
            "\n",
            "   Analyzing license_approved_for_issuance (date field)\n",
            "      CLEAN: Consistent iso_date format\n",
            "\n",
            "   Analyzing date_issued (date field)\n",
            "      CLEAN: Consistent iso_date format\n",
            "\n",
            "   Analyzing license_start_date (date field)\n",
            "      CLEAN: Consistent iso_date format\n",
            "\n",
            "   Analyzing expiration_date (date field)\n",
            "      CLEAN: Consistent iso_date format\n",
            "\n",
            "   Analyzing license_status (text field)\n",
            "\n",
            "   Analyzing ssa (text field)\n",
            "\n",
            "   Analyzing license_status_change_date (date field)\n",
            "      CLEAN: Consistent iso_date format\n",
            "\n",
            "   Analyzing location_latitude (text field)\n",
            "\n",
            "   Analyzing location_longitude (text field)\n",
            "\n",
            "   Analyzing location_human_address (text field)\n",
            "\n",
            "   DATASET SUMMARY: 1 contamination types detected\n",
            "   CLEANING ACTIONS: 1 recommended\n",
            "\n",
            "BUILDING PERMITS CONTAMINATION SCAN\n",
            "--------------------------------------------------\n",
            "\n",
            "   Analyzing id (text field)\n",
            "\n",
            "   Analyzing permit_ (text field)\n",
            "\n",
            "   Analyzing permit_status (text field)\n",
            "\n",
            "   Analyzing permit_milestone (text field)\n",
            "\n",
            "   Analyzing permit_type (text field)\n",
            "      NON-ASCII: 5407 records with non-ASCII characters\n",
            "\n",
            "   Analyzing review_type (text field)\n",
            "\n",
            "   Analyzing application_start_date (date field)\n",
            "      CLEAN: Consistent iso_date format\n",
            "\n",
            "   Analyzing issue_date (date field)\n",
            "      CLEAN: Consistent iso_date format\n",
            "\n",
            "   Analyzing processing_time (text field)\n",
            "\n",
            "   Analyzing street_number (text field)\n",
            "\n",
            "   Analyzing street_direction (text field)\n",
            "\n",
            "   Analyzing street_name (text field)\n",
            "\n",
            "   Analyzing community_area (expected: numeric)\n",
            "      CLEAN: All 8647 values are properly numeric\n",
            "\n",
            "   Analyzing work_type (text field)\n",
            "\n",
            "   Analyzing work_description (text field)\n",
            "      CONTROL CHARS: 1 records with control characters\n",
            "      NON-ASCII: 49 records with non-ASCII characters\n",
            "\n",
            "   DATASET SUMMARY: 3 contamination types detected\n",
            "\n",
            "CTA BOARDINGS CONTAMINATION SCAN\n",
            "--------------------------------------------------\n",
            "\n",
            "   Analyzing service_date (date field)\n",
            "      CLEAN: Consistent iso_date format\n",
            "\n",
            "   DATASET SUMMARY: 0 contamination types detected\n"
          ]
        }
      ],
      "source": [
        "# Dedicated Data Contamination Analyzer\n",
        "def analyze_data_contamination():\n",
        "    \"\"\"Deep analysis of data contamination and mixed types.\"\"\"\n",
        "    print(\"DATA CONTAMINATION ANALYSIS\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    contamination_report = {}\n",
        "\n",
        "    for dataset_name, df in datasets.items():\n",
        "        print(f\"\\n{dataset_name.upper().replace('_', ' ')} CONTAMINATION SCAN\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        dataset_contamination = {\n",
        "            'mixed_types': [],\n",
        "            'special_chars': [],\n",
        "            'encoding_issues': [],\n",
        "            'format_inconsistencies': [],\n",
        "            'recommendations': []\n",
        "        }\n",
        "\n",
        "        for field in df.columns:\n",
        "            non_null = df[field].dropna()\n",
        "            if len(non_null) == 0:\n",
        "                continue\n",
        "\n",
        "            str_values = non_null.astype(str)\n",
        "\n",
        "            # 1. MIXED TYPE DETECTION\n",
        "            if field in ['community_area', 'ward', 'precinct', 'zip_code']:\n",
        "                print(f\"\\n   Analyzing {field} (expected: numeric)\")\n",
        "\n",
        "                # Pattern analysis\n",
        "                pure_numeric = str_values.str.match(r'^\\d+$', na=False)\n",
        "                has_quotes = str_values.str.contains(r\"['\\\"]\", na=False)\n",
        "                has_spaces = str_values.str.contains(r'^\\s|\\s$', na=False)\n",
        "                has_letters = str_values.str.contains(r'[a-zA-Z]', na=False)\n",
        "                has_special = str_values.str.contains(r'[^0-9\\.\\-]', na=False)\n",
        "\n",
        "                contamination_found = False\n",
        "\n",
        "                if has_quotes.sum() > 0:\n",
        "                    contamination_found = True\n",
        "                    quote_examples = str_values[has_quotes].head(3).tolist()\n",
        "                    print(f\"      QUOTES: {has_quotes.sum()} records with quotes - {quote_examples}\")\n",
        "                    dataset_contamination['special_chars'].append({\n",
        "                        'field': field, 'type': 'quotes', 'count': has_quotes.sum(), 'examples': quote_examples\n",
        "                    })\n",
        "\n",
        "                if has_spaces.sum() > 0:\n",
        "                    contamination_found = True\n",
        "                    space_examples = str_values[has_spaces].head(3).tolist()\n",
        "                    print(f\"      SPACES: {has_spaces.sum()} records with leading/trailing spaces - {space_examples}\")\n",
        "                    dataset_contamination['special_chars'].append({\n",
        "                        'field': field, 'type': 'spaces', 'count': has_spaces.sum(), 'examples': space_examples\n",
        "                    })\n",
        "\n",
        "                if has_letters.sum() > 0:\n",
        "                    contamination_found = True\n",
        "                    letter_examples = str_values[has_letters].head(3).tolist()\n",
        "                    print(f\"      LETTERS: {has_letters.sum()} records with letters - {letter_examples}\")\n",
        "                    dataset_contamination['mixed_types'].append({\n",
        "                        'field': field, 'type': 'letters_in_numeric', 'count': has_letters.sum(), 'examples': letter_examples\n",
        "                    })\n",
        "\n",
        "                if not contamination_found:\n",
        "                    print(f\"      CLEAN: All {len(str_values)} values are properly numeric\")\n",
        "\n",
        "            # 2. TEXT FIELD CONTAMINATION\n",
        "            elif df[field].dtype == 'object':\n",
        "                print(f\"\\n   Analyzing {field} (text field)\")\n",
        "\n",
        "                # Check for embedded numbers where inappropriate\n",
        "                if field in ['legal_name', 'doing_business_as_name', 'address']:\n",
        "                    mostly_numeric = str_values.str.match(r'^\\d+$', na=False)\n",
        "                    if mostly_numeric.sum() > len(str_values) * 0.1:  # More than 10% pure numbers\n",
        "                        numeric_examples = str_values[mostly_numeric].head(3).tolist()\n",
        "                        print(f\"      NUMERIC CONTAMINATION: {mostly_numeric.sum()} purely numeric values in text field\")\n",
        "                        dataset_contamination['mixed_types'].append({\n",
        "                            'field': field, 'type': 'numeric_in_text', 'count': mostly_numeric.sum(), 'examples': numeric_examples\n",
        "                        })\n",
        "\n",
        "                # Check for control characters or unusual encoding\n",
        "                control_chars = str_values.str.contains(r'[\\x00-\\x1f\\x7f-\\x9f]', na=False)\n",
        "                if control_chars.sum() > 0:\n",
        "                    control_examples = str_values[control_chars].head(2).tolist()\n",
        "                    print(f\"      CONTROL CHARS: {control_chars.sum()} records with control characters\")\n",
        "                    dataset_contamination['encoding_issues'].append({\n",
        "                        'field': field, 'type': 'control_chars', 'count': control_chars.sum(), 'examples': control_examples\n",
        "                    })\n",
        "\n",
        "                # Check for mixed encoding (non-ASCII where not expected)\n",
        "                try:\n",
        "                    ascii_only = str_values.str.encode('ascii', errors='ignore').str.decode('ascii')\n",
        "                    non_ascii = str_values != ascii_only\n",
        "                    if non_ascii.sum() > 0 and field not in ['legal_name', 'doing_business_as_name']:  # Names can have accents\n",
        "                        non_ascii_examples = str_values[non_ascii].head(2).tolist()\n",
        "                        print(f\"      NON-ASCII: {non_ascii.sum()} records with non-ASCII characters\")\n",
        "                        dataset_contamination['encoding_issues'].append({\n",
        "                            'field': field, 'type': 'non_ascii', 'count': non_ascii.sum(), 'examples': non_ascii_examples\n",
        "                        })\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # 3. DATE FIELD CONTAMINATION\n",
        "            elif field in SchemaManager.get_date_fields(dataset_name):\n",
        "                print(f\"\\n   Analyzing {field} (date field)\")\n",
        "\n",
        "                # Check for multiple date formats\n",
        "                formats = {\n",
        "                    'iso_date': str_values.str.match(r'^\\d{4}-\\d{2}-\\d{2}$', na=False),\n",
        "                    'us_date': str_values.str.match(r'^\\d{1,2}/\\d{1,2}/\\d{4}$', na=False),\n",
        "                    'datetime': str_values.str.contains(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}', na=False),\n",
        "                    'timestamp': str_values.str.match(r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}', na=False)\n",
        "                }\n",
        "\n",
        "                format_counts = {fmt: mask.sum() for fmt, mask in formats.items()}\n",
        "                active_formats = {fmt: count for fmt, count in format_counts.items() if count > 0}\n",
        "\n",
        "                if len(active_formats) > 1:\n",
        "                    print(f\"      FORMAT MIX: Multiple date formats detected - {active_formats}\")\n",
        "                    dataset_contamination['format_inconsistencies'].append({\n",
        "                        'field': field, 'type': 'mixed_date_formats', 'formats': active_formats\n",
        "                    })\n",
        "                elif len(active_formats) == 0:\n",
        "                    unrecognized_examples = str_values.head(3).tolist()\n",
        "                    print(f\"      UNRECOGNIZED: Date format not recognized - {unrecognized_examples}\")\n",
        "                    dataset_contamination['format_inconsistencies'].append({\n",
        "                        'field': field, 'type': 'unrecognized_date_format', 'examples': unrecognized_examples\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"      CLEAN: Consistent {list(active_formats.keys())[0]} format\")\n",
        "\n",
        "        # Generate cleaning recommendations\n",
        "        recommendations = []\n",
        "\n",
        "        for item in dataset_contamination['special_chars']:\n",
        "            if item['type'] == 'quotes':\n",
        "                recommendations.append(f\"Strip quotes from {item['field']}: df['{item['field']}'].str.replace(r'[\\\\'\\\\\\\"]', '', regex=True)\")\n",
        "            elif item['type'] == 'spaces':\n",
        "                recommendations.append(f\"Strip whitespace from {item['field']}: df['{item['field']}'].str.strip()\")\n",
        "\n",
        "        for item in dataset_contamination['mixed_types']:\n",
        "            if item['type'] == 'letters_in_numeric':\n",
        "                recommendations.append(f\"Convert {item['field']} to numeric: pd.to_numeric(df['{item['field']}'], errors='coerce')\")\n",
        "\n",
        "        dataset_contamination['recommendations'] = recommendations\n",
        "        contamination_report[dataset_name] = dataset_contamination\n",
        "\n",
        "        # Summary for this dataset\n",
        "        total_issues = (len(dataset_contamination['mixed_types']) +\n",
        "                       len(dataset_contamination['special_chars']) +\n",
        "                       len(dataset_contamination['encoding_issues']) +\n",
        "                       len(dataset_contamination['format_inconsistencies']))\n",
        "\n",
        "        print(f\"\\n   DATASET SUMMARY: {total_issues} contamination types detected\")\n",
        "        if recommendations:\n",
        "            print(f\"   CLEANING ACTIONS: {len(recommendations)} recommended\")\n",
        "\n",
        "    return contamination_report\n",
        "\n",
        "# Run contamination analysis\n",
        "contamination_analysis = analyze_data_contamination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUTOMATED ANOMALY DETECTION\n",
            "==================================================\n",
            "\n",
            "BUSINESS LICENSES ANOMALIES\n",
            "----------------------------------------\n",
            "   OUTLIERS in license_id: 182 records (8.9%)\n",
            "   OUTLIERS in account_number: 357 records (17.5%)\n",
            "   OUTLIERS in site_number: 344 records (16.9%)\n",
            "   OUTLIERS in license_number: 182 records (8.9%)\n",
            "   FUTURE DATES in expiration_date: 221 records\n",
            "   ALL CAPS TEXT in legal_name: 1833 records\n",
            "   ALL CAPS TEXT in doing_business_as_name: 1677 records\n",
            "   LONG TEXT in business_activity: 6 records (max: 480 chars)\n",
            "   ALL CAPS TEXT in address: 2040 records\n",
            "   ALL CAPS TEXT in city: 2029 records\n",
            "   ALL CAPS TEXT in community_area_name: 1676 records\n",
            "   ALL CAPS TEXT in neighborhood: 1698 records\n",
            "\n",
            "BUILDING PERMITS ANOMALIES\n",
            "----------------------------------------\n",
            "   OUTLIERS in building_fee_paid: 1004 records (11.6%)\n",
            "   OUTLIERS in zoning_fee_paid: 1051 records (12.2%)\n",
            "   OUTLIERS in other_fee_paid: 353 records (4.1%)\n",
            "   OUTLIERS in subtotal_paid: 1044 records (12.1%)\n",
            "   OUTLIERS in building_fee_unpaid: 329 records (3.8%)\n",
            "   OUTLIERS in zoning_fee_unpaid: 32 records (0.4%)\n",
            "   OUTLIERS in other_fee_unpaid: 78 records (0.9%)\n",
            "   OUTLIERS in subtotal_unpaid: 367 records (4.2%)\n",
            "   OUTLIERS in building_fee_waived: 356 records (4.1%)\n",
            "   OUTLIERS in building_fee_subtotal: 1107 records (12.8%)\n",
            "   OUTLIERS in zoning_fee_subtotal: 1090 records (12.6%)\n",
            "   OUTLIERS in other_fee_subtotal: 428 records (4.9%)\n",
            "   OUTLIERS in zoning_fee_waived: 106 records (1.2%)\n",
            "   OUTLIERS in other_fee_waived: 149 records (1.7%)\n",
            "   OUTLIERS in subtotal_waived: 454 records (5.3%)\n",
            "   OUTLIERS in total_fee: 1258 records (14.5%)\n",
            "   ALL CAPS TEXT in permit_status: 7305 records\n",
            "   ALL CAPS TEXT in permit_milestone: 7305 records\n",
            "   ALL CAPS TEXT in permit_type: 8647 records\n",
            "   ALL CAPS TEXT in review_type: 8647 records\n",
            "   ALL CAPS TEXT in street_name: 8643 records\n",
            "   LONG TEXT in work_description: 85 records (max: 1000 chars)\n",
            "   ALL CAPS TEXT in work_description: 8646 records\n",
            "\n",
            "CTA BOARDINGS ANOMALIES\n",
            "----------------------------------------\n",
            "   No significant anomalies detected\n",
            "\n",
            "ANOMALY DETECTION SUMMARY\n",
            "------------------------------\n",
            "   Total anomaly types detected: 35\n",
            "   High severity issues: 12\n",
            "   Datasets with anomalies: 2\n"
          ]
        }
      ],
      "source": [
        "# Automated Anomaly Detection\n",
        "def detect_data_anomalies():\n",
        "    \"\"\"Schema-aware anomaly detection across all datasets.\"\"\"\n",
        "    print(\"AUTOMATED ANOMALY DETECTION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    all_anomalies = {}\n",
        "\n",
        "    for dataset_name, df in datasets.items():\n",
        "        print(f\"\\n{dataset_name.upper().replace('_', ' ')} ANOMALIES\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"   Dataset is empty - skipping analysis\")\n",
        "            continue\n",
        "\n",
        "        dataset_anomalies = []\n",
        "\n",
        "        # Numeric field outliers using IQR method\n",
        "        numeric_fields = df.select_dtypes(include=[np.number]).columns\n",
        "        schema_numeric = [f for f in SchemaManager.get_field_names(dataset_name) if f in numeric_fields]\n",
        "\n",
        "        for field in schema_numeric:\n",
        "            if field in df.columns and df[field].notna().sum() > 5:  # Need at least 5 values\n",
        "                Q1 = df[field].quantile(0.25)\n",
        "                Q3 = df[field].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "                outliers = df[(df[field] < lower_bound) | (df[field] > upper_bound)]\n",
        "                if len(outliers) > 0:\n",
        "                    outlier_pct = len(outliers) / len(df) * 100\n",
        "                    dataset_anomalies.append({\n",
        "                        'field': field,\n",
        "                        'type': 'outliers',\n",
        "                        'count': len(outliers),\n",
        "                        'percentage': outlier_pct,\n",
        "                        'severity': 'HIGH' if outlier_pct > 5 else 'MEDIUM' if outlier_pct > 1 else 'LOW'\n",
        "                    })\n",
        "                    print(f\"   OUTLIERS in {field}: {len(outliers)} records ({outlier_pct:.1f}%)\")\n",
        "\n",
        "        # Date field anomalies\n",
        "        date_fields = SchemaManager.get_date_fields(dataset_name)\n",
        "        for field in date_fields:\n",
        "            if field in df.columns:\n",
        "                date_series = pd.to_datetime(df[field], errors='coerce')\n",
        "\n",
        "                # Future dates (beyond reasonable business timeframe)\n",
        "                future_cutoff = datetime.now() + timedelta(days=365*2)  # 2 years in future\n",
        "                future_dates = date_series[date_series > future_cutoff]\n",
        "                if len(future_dates) > 0:\n",
        "                    dataset_anomalies.append({\n",
        "                        'field': field,\n",
        "                        'type': 'future_dates',\n",
        "                        'count': len(future_dates),\n",
        "                        'examples': future_dates.head(3).tolist(),\n",
        "                        'severity': 'HIGH'\n",
        "                    })\n",
        "                    print(f\"   FUTURE DATES in {field}: {len(future_dates)} records\")\n",
        "\n",
        "                # Very old dates (potentially data entry errors)\n",
        "                old_cutoff = datetime.now() - timedelta(days=365*20)  # 20 years ago\n",
        "                old_dates = date_series[date_series < old_cutoff]\n",
        "                if len(old_dates) > 0:\n",
        "                    dataset_anomalies.append({\n",
        "                        'field': field,\n",
        "                        'type': 'very_old_dates',\n",
        "                        'count': len(old_dates),\n",
        "                        'examples': old_dates.head(3).tolist(),\n",
        "                        'severity': 'MEDIUM'\n",
        "                    })\n",
        "                    print(f\"   VERY OLD DATES in {field}: {len(old_dates)} records\")\n",
        "\n",
        "        # Text field anomalies\n",
        "        text_fields = df.select_dtypes(include=['object']).columns\n",
        "        for field in text_fields:\n",
        "            if field in df.columns:\n",
        "                # Extremely long text entries\n",
        "                text_lengths = df[field].astype(str).str.len()\n",
        "                long_threshold = text_lengths.quantile(0.99)  # 99th percentile\n",
        "                if long_threshold > 100:  # Only check if reasonable threshold\n",
        "                    long_entries = df[text_lengths > long_threshold]\n",
        "                    if len(long_entries) > 0:\n",
        "                        dataset_anomalies.append({\n",
        "                            'field': field,\n",
        "                            'type': 'unusually_long_text',\n",
        "                            'count': len(long_entries),\n",
        "                            'max_length': text_lengths.max(),\n",
        "                            'severity': 'LOW'\n",
        "                        })\n",
        "                        print(f\"   LONG TEXT in {field}: {len(long_entries)} records (max: {text_lengths.max()} chars)\")\n",
        "\n",
        "                # Suspicious patterns (all caps, all numbers, etc.)\n",
        "                if field not in ['id', 'license_id', 'permit_']:  # Skip ID fields\n",
        "                    all_caps = df[df[field].astype(str).str.isupper() & (df[field].astype(str).str.len() > 5)]\n",
        "                    if len(all_caps) > len(df) * 0.1:  # More than 10% all caps\n",
        "                        dataset_anomalies.append({\n",
        "                            'field': field,\n",
        "                            'type': 'excessive_all_caps',\n",
        "                            'count': len(all_caps),\n",
        "                            'percentage': len(all_caps) / len(df) * 100,\n",
        "                            'severity': 'LOW'\n",
        "                        })\n",
        "                        print(f\"   ALL CAPS TEXT in {field}: {len(all_caps)} records\")\n",
        "\n",
        "        all_anomalies[dataset_name] = dataset_anomalies\n",
        "\n",
        "        if not dataset_anomalies:\n",
        "            print(\"   No significant anomalies detected\")\n",
        "\n",
        "    # Summary\n",
        "    total_anomalies = sum(len(anomalies) for anomalies in all_anomalies.values())\n",
        "    high_severity = sum(1 for anomalies in all_anomalies.values() for a in anomalies if a.get('severity') == 'HIGH')\n",
        "\n",
        "    print(f\"\\nANOMALY DETECTION SUMMARY\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"   Total anomaly types detected: {total_anomalies}\")\n",
        "    print(f\"   High severity issues: {high_severity}\")\n",
        "    print(f\"   Datasets with anomalies: {len([k for k, v in all_anomalies.items() if v])}\")\n",
        "\n",
        "    return all_anomalies\n",
        "\n",
        "# Run anomaly detection\n",
        "anomalies = detect_data_anomalies()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Business Logic Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BUSINESS LOGIC VALIDATION\n",
            "========================================\n",
            "\n",
            "BUSINESS LICENSES BUSINESS RULES\n",
            "------------------------------\n",
            "   DATE LOGIC ERROR: 70 records\n",
            "   GEOGRAPHIC ERROR: 4 records outside Chicago bounds\n",
            "   STATUS ERROR: 2040 records with invalid status\n",
            "\n",
            "BUILDING PERMITS BUSINESS RULES\n",
            "------------------------------\n",
            "   TYPE ERROR: 8647 records with invalid permit type\n",
            "\n",
            "CTA BOARDINGS BUSINESS RULES\n",
            "------------------------------\n",
            "   All business rules validated successfully\n",
            "\n",
            "BUSINESS VALIDATION SUMMARY\n",
            "------------------------------\n",
            "   Total business rule violations: 4\n",
            "   Datasets with violations: 2\n"
          ]
        }
      ],
      "source": [
        "# Business Rule Validation\n",
        "def validate_business_rules():\n",
        "    \"\"\"Chicago-specific business logic validation.\"\"\"\n",
        "    print(\"BUSINESS LOGIC VALIDATION\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    all_business_issues = {}\n",
        "\n",
        "    for dataset_name, df in datasets.items():\n",
        "        print(f\"\\n{dataset_name.upper().replace('_', ' ')} BUSINESS RULES\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"   Dataset is empty - skipping validation\")\n",
        "            continue\n",
        "\n",
        "        issues = []\n",
        "\n",
        "        # Business Licenses specific validation\n",
        "        if dataset_name == 'business_licenses':\n",
        "\n",
        "            # License date logic validation\n",
        "            if all(field in df.columns for field in ['license_start_date', 'expiration_date']):\n",
        "                start_dates = pd.to_datetime(df['license_start_date'], errors='coerce')\n",
        "                exp_dates = pd.to_datetime(df['expiration_date'], errors='coerce')\n",
        "                invalid_dates = df[(start_dates >= exp_dates) & start_dates.notna() & exp_dates.notna()]\n",
        "                if len(invalid_dates) > 0:\n",
        "                    issues.append(f\"Invalid date logic: {len(invalid_dates)} licenses start after expiration\")\n",
        "                    print(f\"   DATE LOGIC ERROR: {len(invalid_dates)} records\")\n",
        "\n",
        "            # Application workflow validation\n",
        "            if all(field in df.columns for field in ['application_created_date', 'license_start_date']):\n",
        "                app_dates = pd.to_datetime(df['application_created_date'], errors='coerce')\n",
        "                start_dates = pd.to_datetime(df['license_start_date'], errors='coerce')\n",
        "                workflow_issues = df[(app_dates > start_dates) & app_dates.notna() & start_dates.notna()]\n",
        "                if len(workflow_issues) > 0:\n",
        "                    issues.append(f\"Workflow logic error: {len(workflow_issues)} licenses start before application\")\n",
        "                    print(f\"   WORKFLOW ERROR: {len(workflow_issues)} records\")\n",
        "\n",
        "            # Chicago geographic bounds validation\n",
        "            if all(field in df.columns for field in ['latitude', 'longitude']):\n",
        "                # Chicago bounds: roughly 41.6-42.1 latitude, -87.9 to -87.5 longitude\n",
        "                lat_series = pd.to_numeric(df['latitude'], errors='coerce')\n",
        "                lon_series = pd.to_numeric(df['longitude'], errors='coerce')\n",
        "\n",
        "                out_of_bounds = df[\n",
        "                    ((lat_series < 41.6) | (lat_series > 42.1) |\n",
        "                     (lon_series < -87.9) | (lon_series > -87.5)) &\n",
        "                    lat_series.notna() & lon_series.notna()\n",
        "                ]\n",
        "                if len(out_of_bounds) > 0:\n",
        "                    issues.append(f\"Geographic bounds: {len(out_of_bounds)} records outside Chicago area\")\n",
        "                    print(f\"   GEOGRAPHIC ERROR: {len(out_of_bounds)} records outside Chicago bounds\")\n",
        "\n",
        "            # License status consistency\n",
        "            if 'license_status' in df.columns:\n",
        "                valid_statuses = ['ISSUED', 'ACTIVE', 'EXPIRED', 'REVOKED', 'CANCELLED']\n",
        "                invalid_status = df[~df['license_status'].isin(valid_statuses)]\n",
        "                if len(invalid_status) > 0:\n",
        "                    unique_invalid = invalid_status['license_status'].unique()\n",
        "                    issues.append(f\"Invalid license status: {list(unique_invalid)}\")\n",
        "                    print(f\"   STATUS ERROR: {len(invalid_status)} records with invalid status\")\n",
        "\n",
        "        # Building Permits specific validation\n",
        "        elif dataset_name == 'building_permits':\n",
        "\n",
        "            # Permit workflow validation\n",
        "            if all(field in df.columns for field in ['application_start_date', 'issue_date']):\n",
        "                app_dates = pd.to_datetime(df['application_start_date'], errors='coerce')\n",
        "                issue_dates = pd.to_datetime(df['issue_date'], errors='coerce')\n",
        "                workflow_issues = df[(app_dates > issue_dates) & app_dates.notna() & issue_dates.notna()]\n",
        "                if len(workflow_issues) > 0:\n",
        "                    issues.append(f\"Permit workflow: {len(workflow_issues)} permits issued before application\")\n",
        "                    print(f\"   WORKFLOW ERROR: {len(workflow_issues)} records\")\n",
        "\n",
        "            # Permit type validation\n",
        "            if 'permit_type' in df.columns:\n",
        "                expected_types = ['PERMIT', 'EASY_PERMIT', 'REVIEW', 'INSPECTION']\n",
        "                invalid_types = df[~df['permit_type'].isin(expected_types)]\n",
        "                if len(invalid_types) > 0:\n",
        "                    unique_invalid = invalid_types['permit_type'].unique()\n",
        "                    issues.append(f\"Invalid permit types: {list(unique_invalid)}\")\n",
        "                    print(f\"   TYPE ERROR: {len(invalid_types)} records with invalid permit type\")\n",
        "\n",
        "        # CTA Boardings specific validation\n",
        "        elif dataset_name == 'cta_boardings':\n",
        "\n",
        "            # Ridership reasonableness check\n",
        "            if 'total_rides' in df.columns:\n",
        "                rides = pd.to_numeric(df['total_rides'], errors='coerce')\n",
        "                # Check for unreasonably high ridership (> 10M per day)\n",
        "                high_ridership = df[rides > 10000000]\n",
        "                if len(high_ridership) > 0:\n",
        "                    issues.append(f\"Unreasonably high ridership: {len(high_ridership)} days with >10M rides\")\n",
        "                    print(f\"   RIDERSHIP ERROR: {len(high_ridership)} records with excessive ridership\")\n",
        "\n",
        "                # Check for negative ridership\n",
        "                negative_rides = df[rides < 0]\n",
        "                if len(negative_rides) > 0:\n",
        "                    issues.append(f\"Negative ridership: {len(negative_rides)} records\")\n",
        "                    print(f\"   NEGATIVE VALUES: {len(negative_rides)} records\")\n",
        "\n",
        "        all_business_issues[dataset_name] = issues\n",
        "\n",
        "        if not issues:\n",
        "            print(\"   All business rules validated successfully\")\n",
        "\n",
        "    # Summary\n",
        "    total_issues = sum(len(issues) for issues in all_business_issues.values())\n",
        "    datasets_with_issues = len([k for k, v in all_business_issues.items() if v])\n",
        "\n",
        "    print(f\"\\nBUSINESS VALIDATION SUMMARY\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"   Total business rule violations: {total_issues}\")\n",
        "    print(f\"   Datasets with violations: {datasets_with_issues}\")\n",
        "\n",
        "    return all_business_issues\n",
        "\n",
        "# Run business rules validation\n",
        "business_issues = validate_business_rules()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Completeness & Consistency Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA COMPLETENESS & CONSISTENCY ANALYSIS\n",
            "==================================================\n",
            "\n",
            "BUSINESS LICENSES COMPLETENESS\n",
            "----------------------------------------\n",
            "   Total fields: 39 (Required: 12, Optional: 27)\n",
            "\n",
            "   REQUIRED FIELDS ANALYSIS:\n",
            "     OK: id - 100% complete\n",
            "     OK: license_id - 100% complete\n",
            "     OK: legal_name - 100% complete\n",
            "     OK: license_description - 100% complete\n",
            "     OK: address - 100% complete\n",
            "     OK: city - 100% complete\n",
            "     OK: state - 100% complete\n",
            "     OK: community_area - 100% complete\n",
            "     OK: community_area_name - 100% complete\n",
            "     OK: application_type - 100% complete\n",
            "     OK: license_start_date - 100% complete\n",
            "     OK: license_status - 100% complete\n",
            "\n",
            "   OPTIONAL FIELDS ANALYSIS:\n",
            "     GOOD: account_number - 100.0% complete\n",
            "     GOOD: application_created_date - 99.9% complete\n",
            "     GOOD: application_requirements_complete - 100.0% complete\n",
            "     GOOD: business_activity - 100.0% complete\n",
            "     GOOD: business_activity_id - 100.0% complete\n",
            "     LOW VALUE: conditional_approval - 0.0% complete (consider dropping)\n",
            "     GOOD: date_issued - 100.0% complete\n",
            "     GOOD: doing_business_as_name - 100.0% complete\n",
            "     GOOD: expiration_date - 99.9% complete\n",
            "     GOOD: latitude - 100.0% complete\n",
            "     GOOD: license_approved_for_issuance - 86.8% complete\n",
            "     GOOD: license_code - 100.0% complete\n",
            "     GOOD: license_number - 100.0% complete\n",
            "     LOW VALUE: license_status_change_date - 0.4% complete (consider dropping)\n",
            "     GOOD: location_human_address - 100.0% complete\n",
            "     GOOD: location_latitude - 100.0% complete\n",
            "     GOOD: location_longitude - 100.0% complete\n",
            "     GOOD: longitude - 100.0% complete\n",
            "     GOOD: neighborhood - 100.0% complete\n",
            "     GOOD: payment_date - 99.5% complete\n",
            "     GOOD: police_district - 100.0% complete\n",
            "     GOOD: precinct - 100.0% complete\n",
            "     GOOD: site_number - 100.0% complete\n",
            "     GOOD: ssa - 100.0% complete\n",
            "     GOOD: ward - 100.0% complete\n",
            "     GOOD: ward_precinct - 100.0% complete\n",
            "     GOOD: zip_code - 100.0% complete\n",
            "\n",
            "   CONSISTENCY ANALYSIS:\n",
            "     TYPE ISSUE: zip_code - appears numeric but stored as text\n",
            "     TYPE ISSUE: ward - appears numeric but stored as text\n",
            "     TYPE ISSUE: precinct - appears numeric but stored as text\n",
            "     TYPE ISSUE: police_district - appears numeric but stored as text\n",
            "     TYPE ISSUE: community_area - appears numeric but stored as text\n",
            "\n",
            "   SUMMARY:\n",
            "     Critical issues: 0\n",
            "     Low-value fields: 2\n",
            "     Consistency issues: 5\n",
            "     Cleaning recommendations: 2\n",
            "\n",
            "BUILDING PERMITS COMPLETENESS\n",
            "----------------------------------------\n",
            "   Total fields: 31 (Required: 5, Optional: 26)\n",
            "\n",
            "   REQUIRED FIELDS ANALYSIS:\n",
            "     OK: id - 100% complete\n",
            "     OK: permit_ - 100% complete\n",
            "     OK: permit_status - 100% complete\n",
            "     OK: permit_type - 100% complete\n",
            "     OK: issue_date - 100% complete\n",
            "\n",
            "   OPTIONAL FIELDS ANALYSIS:\n",
            "     GOOD: application_start_date - 100.0% complete\n",
            "     GOOD: building_fee_paid - 100.0% complete\n",
            "     GOOD: building_fee_subtotal - 100.0% complete\n",
            "     GOOD: building_fee_unpaid - 100.0% complete\n",
            "     GOOD: building_fee_waived - 100.0% complete\n",
            "     GOOD: community_area - 100.0% complete\n",
            "     GOOD: other_fee_paid - 100.0% complete\n",
            "     GOOD: other_fee_subtotal - 100.0% complete\n",
            "     GOOD: other_fee_unpaid - 100.0% complete\n",
            "     GOOD: other_fee_waived - 100.0% complete\n",
            "     GOOD: permit_milestone - 100.0% complete\n",
            "     GOOD: processing_time - 100.0% complete\n",
            "     GOOD: review_type - 100.0% complete\n",
            "     GOOD: street_direction - 100.0% complete\n",
            "     GOOD: street_name - 100.0% complete\n",
            "     GOOD: street_number - 100.0% complete\n",
            "     GOOD: subtotal_paid - 100.0% complete\n",
            "     GOOD: subtotal_unpaid - 100.0% complete\n",
            "     GOOD: subtotal_waived - 100.0% complete\n",
            "     GOOD: total_fee - 100.0% complete\n",
            "     GOOD: work_description - 100.0% complete\n",
            "     GOOD: work_type - 100.0% complete\n",
            "     GOOD: zoning_fee_paid - 100.0% complete\n",
            "     GOOD: zoning_fee_subtotal - 100.0% complete\n",
            "     GOOD: zoning_fee_unpaid - 100.0% complete\n",
            "     GOOD: zoning_fee_waived - 100.0% complete\n",
            "\n",
            "   CONSISTENCY ANALYSIS:\n",
            "     TYPE ISSUE: processing_time - appears numeric but stored as text\n",
            "     TYPE ISSUE: street_number - appears numeric but stored as text\n",
            "     TYPE ISSUE: community_area - appears numeric but stored as text\n",
            "\n",
            "   SUMMARY:\n",
            "     Critical issues: 0\n",
            "     Low-value fields: 0\n",
            "     Consistency issues: 3\n",
            "     Cleaning recommendations: 0\n",
            "\n",
            "CTA BOARDINGS COMPLETENESS\n",
            "----------------------------------------\n",
            "   Total fields: 2 (Required: 2, Optional: 0)\n",
            "\n",
            "   REQUIRED FIELDS ANALYSIS:\n",
            "     OK: service_date - 100% complete\n",
            "     OK: total_rides - 100% complete\n",
            "\n",
            "   OPTIONAL FIELDS ANALYSIS:\n",
            "\n",
            "   CONSISTENCY ANALYSIS:\n",
            "\n",
            "   SUMMARY:\n",
            "     Critical issues: 0\n",
            "     Low-value fields: 0\n",
            "     Consistency issues: 0\n",
            "     Cleaning recommendations: 0\n",
            "\n",
            "OVERALL COMPLETENESS SUMMARY\n",
            "-----------------------------------\n",
            "   Total cleaning recommendations: 2\n",
            "   Total consistency issues: 8\n",
            "   Datasets requiring attention: 1\n"
          ]
        }
      ],
      "source": [
        "# Data Completeness Analysis\n",
        "def analyze_data_completeness():\n",
        "    \"\"\"Comprehensive completeness analysis with cleaning recommendations.\"\"\"\n",
        "    print(\"DATA COMPLETENESS & CONSISTENCY ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    completeness_report = {}\n",
        "\n",
        "    for dataset_name, df in datasets.items():\n",
        "        print(f\"\\n{dataset_name.upper().replace('_', ' ')} COMPLETENESS\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"   Dataset is empty - skipping analysis\")\n",
        "            continue\n",
        "\n",
        "        required_fields = SchemaManager.get_required_fields(dataset_name)\n",
        "        all_fields = SchemaManager.get_field_names(dataset_name)\n",
        "        optional_fields = set(all_fields) - set(required_fields)\n",
        "\n",
        "        report = {\n",
        "            'required_field_completeness': {},\n",
        "            'optional_field_completeness': {},\n",
        "            'cleaning_recommendations': [],\n",
        "            'consistency_issues': []\n",
        "        }\n",
        "\n",
        "        print(f\"   Total fields: {len(all_fields)} (Required: {len(required_fields)}, Optional: {len(optional_fields)})\")\n",
        "\n",
        "        # Required fields analysis - must be 100% complete\n",
        "        print(f\"\\n   REQUIRED FIELDS ANALYSIS:\")\n",
        "        critical_issues = 0\n",
        "        for field in required_fields:\n",
        "            if field in df.columns:\n",
        "                completeness = (df[field].notna().sum() / len(df)) * 100\n",
        "                report['required_field_completeness'][field] = completeness\n",
        "\n",
        "                if completeness < 100:\n",
        "                    critical_issues += 1\n",
        "                    missing_count = df[field].isna().sum()\n",
        "                    print(f\"     CRITICAL: {field} - {completeness:.1f}% complete ({missing_count} missing)\")\n",
        "\n",
        "                    # Generate imputation strategy\n",
        "                    if df[field].dtype in ['int64', 'float64']:\n",
        "                        strategy = 'median_imputation'\n",
        "                        suggestion = f\"Use median value: {df[field].median():.2f}\"\n",
        "                    elif df[field].dtype == 'object':\n",
        "                        mode_val = df[field].mode()\n",
        "                        if len(mode_val) > 0:\n",
        "                            strategy = 'mode_imputation'\n",
        "                            suggestion = f\"Use most common value: '{mode_val.iloc[0]}'\"\n",
        "                        else:\n",
        "                            strategy = 'default_value'\n",
        "                            suggestion = \"Assign default value or 'UNKNOWN'\"\n",
        "                    else:\n",
        "                        strategy = 'investigation_required'\n",
        "                        suggestion = \"Manual investigation needed\"\n",
        "\n",
        "                    report['cleaning_recommendations'].append({\n",
        "                        'field': field,\n",
        "                        'action': 'CRITICAL_IMPUTATION',\n",
        "                        'strategy': strategy,\n",
        "                        'missing_count': missing_count,\n",
        "                        'missing_pct': 100 - completeness,\n",
        "                        'suggestion': suggestion\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"     OK: {field} - 100% complete\")\n",
        "            else:\n",
        "                print(f\"     MISSING: {field} - Field not present in dataset\")\n",
        "                report['cleaning_recommendations'].append({\n",
        "                    'field': field,\n",
        "                    'action': 'FIELD_MISSING',\n",
        "                    'strategy': 'data_source_investigation',\n",
        "                    'suggestion': 'Check data source - required field completely absent'\n",
        "                })\n",
        "\n",
        "        # Optional fields analysis\n",
        "        print(f\"\\n   OPTIONAL FIELDS ANALYSIS:\")\n",
        "        low_value_fields = 0\n",
        "        for field in sorted(optional_fields):\n",
        "            if field in df.columns:\n",
        "                completeness = (df[field].notna().sum() / len(df)) * 100\n",
        "                report['optional_field_completeness'][field] = completeness\n",
        "\n",
        "                if completeness < 25:  # Very sparse\n",
        "                    low_value_fields += 1\n",
        "                    print(f\"     LOW VALUE: {field} - {completeness:.1f}% complete (consider dropping)\")\n",
        "                    report['cleaning_recommendations'].append({\n",
        "                        'field': field,\n",
        "                        'action': 'CONSIDER_DROPPING',\n",
        "                        'missing_pct': 100 - completeness,\n",
        "                        'suggestion': f'Only {completeness:.1f}% populated - assess business value'\n",
        "                    })\n",
        "                elif completeness < 75:  # Moderately sparse\n",
        "                    print(f\"     SPARSE: {field} - {completeness:.1f}% complete\")\n",
        "                else:\n",
        "                    print(f\"     GOOD: {field} - {completeness:.1f}% complete\")\n",
        "\n",
        "        # Consistency checks\n",
        "        print(f\"\\n   CONSISTENCY ANALYSIS:\")\n",
        "\n",
        "        # Check for mixed data types in object columns\n",
        "        for field in df.select_dtypes(include=['object']).columns:\n",
        "            sample_values = df[field].dropna().head(100)\n",
        "            if len(sample_values) > 0:\n",
        "                # Check for numeric values in text fields\n",
        "                numeric_like = sample_values.astype(str).str.isnumeric().sum()\n",
        "                if numeric_like > len(sample_values) * 0.8:  # 80% numeric-like\n",
        "                    report['consistency_issues'].append(f\"{field}: Appears numeric but stored as text\")\n",
        "                    print(f\"     TYPE ISSUE: {field} - appears numeric but stored as text\")\n",
        "\n",
        "                # Check for mixed case inconsistency\n",
        "                if field in ['city', 'state', 'license_status', 'permit_status']:\n",
        "                    case_variations = len(sample_values.str.upper().unique())\n",
        "                    actual_variations = len(sample_values.unique())\n",
        "                    if case_variations < actual_variations:\n",
        "                        report['consistency_issues'].append(f\"{field}: Mixed case inconsistency\")\n",
        "                        print(f\"     CASE ISSUE: {field} - inconsistent capitalization\")\n",
        "\n",
        "        # Date format consistency\n",
        "        date_fields = SchemaManager.get_date_fields(dataset_name)\n",
        "        for field in date_fields:\n",
        "            if field in df.columns:\n",
        "                sample_dates = df[field].dropna().astype(str).head(50)\n",
        "                if len(sample_dates) > 0:\n",
        "                    # Check for multiple date formats\n",
        "                    formats_found = []\n",
        "                    for date_str in sample_dates:\n",
        "                        if '-' in date_str and len(date_str) == 10:\n",
        "                            formats_found.append('YYYY-MM-DD')\n",
        "                        elif '/' in date_str:\n",
        "                            formats_found.append('MM/DD/YYYY')\n",
        "                        elif len(date_str) > 15:\n",
        "                            formats_found.append('DATETIME')\n",
        "\n",
        "                    unique_formats = set(formats_found)\n",
        "                    if len(unique_formats) > 1:\n",
        "                        report['consistency_issues'].append(f\"{field}: Multiple date formats detected\")\n",
        "                        print(f\"     DATE FORMAT: {field} - mixed formats: {unique_formats}\")\n",
        "\n",
        "        completeness_report[dataset_name] = report\n",
        "\n",
        "        # Field summary\n",
        "        print(f\"\\n   SUMMARY:\")\n",
        "        print(f\"     Critical issues: {critical_issues}\")\n",
        "        print(f\"     Low-value fields: {low_value_fields}\")\n",
        "        print(f\"     Consistency issues: {len(report['consistency_issues'])}\")\n",
        "        print(f\"     Cleaning recommendations: {len(report['cleaning_recommendations'])}\")\n",
        "\n",
        "    # Overall summary\n",
        "    total_recommendations = sum(len(report['cleaning_recommendations']) for report in completeness_report.values())\n",
        "    total_consistency_issues = sum(len(report['consistency_issues']) for report in completeness_report.values())\n",
        "\n",
        "    print(f\"\\nOVERALL COMPLETENESS SUMMARY\")\n",
        "    print(\"-\" * 35)\n",
        "    print(f\"   Total cleaning recommendations: {total_recommendations}\")\n",
        "    print(f\"   Total consistency issues: {total_consistency_issues}\")\n",
        "    print(f\"   Datasets requiring attention: {len([k for k, v in completeness_report.items() if v['cleaning_recommendations']])}\")\n",
        "\n",
        "    return completeness_report\n",
        "\n",
        "# Run completeness analysis\n",
        "completeness_analysis = analyze_data_completeness()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Quality Check Caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SAVING QUALITY ANALYSIS RESULTS\n",
            "==================================================\n",
            "   üìÅ Created batch folder: data_quality_batch_20250901_153504\n",
            "   ‚úÖ Quality matrix: quality_matrix.pkl\n",
            "   ‚úÖ Contamination analysis: contamination_analysis.pkl\n",
            "   ‚úÖ Anomaly detection: anomaly_detection.pkl\n",
            "   ‚úÖ Business validation: business_validation.pkl\n",
            "   ‚úÖ Completeness analysis: completeness_analysis.pkl\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'summary' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 231\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m summary, batch_name\n\u001b[32m    230\u001b[39m \u001b[38;5;66;03m# Run the enhanced caching function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m analysis_summary, batch_id = \u001b[43msave_quality_analysis_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36msave_quality_analysis_results\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚úÖ Completeness analysis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompleteness_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# 6. Create Comprehensive Summary with Analysis Insights\u001b[39;00m\n\u001b[32m     78\u001b[39m summary = {\n\u001b[32m     79\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbatch_info\u001b[39m\u001b[33m'\u001b[39m: metadata,\n\u001b[32m     80\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mquality_summary\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m     81\u001b[39m         \u001b[33m'\u001b[39m\u001b[33moverall_score\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(quality_matrix[\u001b[33m'\u001b[39m\u001b[33mquality_score\u001b[39m\u001b[33m'\u001b[39m].mean()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quality_matrix.empty \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m     82\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdata_purity_score\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(quality_matrix[\u001b[33m'\u001b[39m\u001b[33mcontamination_score\u001b[39m\u001b[33m'\u001b[39m].mean()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quality_matrix.empty \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m     83\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcontaminated_fields\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(quality_matrix[quality_matrix[\u001b[33m'\u001b[39m\u001b[33mcontamination_score\u001b[39m\u001b[33m'\u001b[39m] < \u001b[32m0.95\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quality_matrix.empty \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m     84\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcritical_issues\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(quality_matrix[(quality_matrix[\u001b[33m'\u001b[39m\u001b[33mis_required\u001b[39m\u001b[33m'\u001b[39m]) & (quality_matrix[\u001b[33m'\u001b[39m\u001b[33mquality_score\u001b[39m\u001b[33m'\u001b[39m] < \u001b[32m0.8\u001b[39m)]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quality_matrix.empty \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m     85\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mexcellent_fields\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(quality_matrix[quality_matrix[\u001b[33m'\u001b[39m\u001b[33mquality_score\u001b[39m\u001b[33m'\u001b[39m] >= \u001b[32m0.95\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quality_matrix.empty \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     86\u001b[39m     },\n\u001b[32m     87\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcontamination_summary\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m     88\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_contamination_types\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\n\u001b[32m     89\u001b[39m             \u001b[38;5;28mlen\u001b[39m(report.get(\u001b[33m'\u001b[39m\u001b[33mmixed_types\u001b[39m\u001b[33m'\u001b[39m, [])) +\n\u001b[32m     90\u001b[39m             \u001b[38;5;28mlen\u001b[39m(report.get(\u001b[33m'\u001b[39m\u001b[33mspecial_chars\u001b[39m\u001b[33m'\u001b[39m, [])) +\n\u001b[32m     91\u001b[39m             \u001b[38;5;28mlen\u001b[39m(report.get(\u001b[33m'\u001b[39m\u001b[33mencoding_issues\u001b[39m\u001b[33m'\u001b[39m, [])) +\n\u001b[32m     92\u001b[39m             \u001b[38;5;28mlen\u001b[39m(report.get(\u001b[33m'\u001b[39m\u001b[33mformat_inconsistencies\u001b[39m\u001b[33m'\u001b[39m, []))\n\u001b[32m     93\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m report \u001b[38;5;129;01min\u001b[39;00m contamination_analysis.values()\n\u001b[32m     94\u001b[39m         ),\n\u001b[32m     95\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdatasets_with_contamination\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m([k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m contamination_analysis.items() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([\n\u001b[32m     96\u001b[39m             v.get(\u001b[33m'\u001b[39m\u001b[33mmixed_types\u001b[39m\u001b[33m'\u001b[39m), v.get(\u001b[33m'\u001b[39m\u001b[33mspecial_chars\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     97\u001b[39m             v.get(\u001b[33m'\u001b[39m\u001b[33mencoding_issues\u001b[39m\u001b[33m'\u001b[39m), v.get(\u001b[33m'\u001b[39m\u001b[33mformat_inconsistencies\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     98\u001b[39m         ])]),\n\u001b[32m     99\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmixed_type_issues\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(report.get(\u001b[33m'\u001b[39m\u001b[33mmixed_types\u001b[39m\u001b[33m'\u001b[39m, [])) \u001b[38;5;28;01mfor\u001b[39;00m report \u001b[38;5;129;01min\u001b[39;00m contamination_analysis.values()),\n\u001b[32m    100\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mspecial_char_issues\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(report.get(\u001b[33m'\u001b[39m\u001b[33mspecial_chars\u001b[39m\u001b[33m'\u001b[39m, [])) \u001b[38;5;28;01mfor\u001b[39;00m report \u001b[38;5;129;01min\u001b[39;00m contamination_analysis.values()),\n\u001b[32m    101\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mencoding_issues\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(report.get(\u001b[33m'\u001b[39m\u001b[33mencoding_issues\u001b[39m\u001b[33m'\u001b[39m, [])) \u001b[38;5;28;01mfor\u001b[39;00m report \u001b[38;5;129;01min\u001b[39;00m contamination_analysis.values())\n\u001b[32m    102\u001b[39m     },\n\u001b[32m    103\u001b[39m     \u001b[33m'\u001b[39m\u001b[33manomaly_summary\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m    104\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_anomaly_types\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(anomaly_list) \u001b[38;5;28;01mfor\u001b[39;00m anomaly_list \u001b[38;5;129;01min\u001b[39;00m anomalies.values()),\n\u001b[32m    105\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mhigh_severity_issues\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m anomaly_list \u001b[38;5;129;01min\u001b[39;00m anomalies.values() \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m anomaly_list \u001b[38;5;28;01mif\u001b[39;00m a.get(\u001b[33m'\u001b[39m\u001b[33mseverity\u001b[39m\u001b[33m'\u001b[39m) == \u001b[33m'\u001b[39m\u001b[33mHIGH\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    106\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmedium_severity_issues\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m anomaly_list \u001b[38;5;129;01min\u001b[39;00m anomalies.values() \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m anomaly_list \u001b[38;5;28;01mif\u001b[39;00m a.get(\u001b[33m'\u001b[39m\u001b[33mseverity\u001b[39m\u001b[33m'\u001b[39m) == \u001b[33m'\u001b[39m\u001b[33mMEDIUM\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    107\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlow_severity_issues\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m anomaly_list \u001b[38;5;129;01min\u001b[39;00m anomalies.values() \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m anomaly_list \u001b[38;5;28;01mif\u001b[39;00m a.get(\u001b[33m'\u001b[39m\u001b[33mseverity\u001b[39m\u001b[33m'\u001b[39m) == \u001b[33m'\u001b[39m\u001b[33mLOW\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    108\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdatasets_with_anomalies\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m([k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m anomalies.items() \u001b[38;5;28;01mif\u001b[39;00m v])\n\u001b[32m    109\u001b[39m     },\n\u001b[32m    110\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbusiness_rules_summary\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m    111\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_violations\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(issues) \u001b[38;5;28;01mfor\u001b[39;00m issues \u001b[38;5;129;01min\u001b[39;00m business_issues.values()),\n\u001b[32m    112\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdatasets_with_violations\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m([k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m business_issues.items() \u001b[38;5;28;01mif\u001b[39;00m v]),\n\u001b[32m    113\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mviolation_types\u001b[39m\u001b[33m'\u001b[39m: [issue \u001b[38;5;28;01mfor\u001b[39;00m issues \u001b[38;5;129;01min\u001b[39;00m business_issues.values() \u001b[38;5;28;01mfor\u001b[39;00m issue \u001b[38;5;129;01min\u001b[39;00m issues]\n\u001b[32m    114\u001b[39m     },\n\u001b[32m    115\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcompleteness_summary\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m    116\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_recommendations\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(report[\u001b[33m'\u001b[39m\u001b[33mcleaning_recommendations\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m report \u001b[38;5;129;01min\u001b[39;00m completeness_analysis.values()),\n\u001b[32m    117\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_consistency_issues\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(report[\u001b[33m'\u001b[39m\u001b[33mconsistency_issues\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m report \u001b[38;5;129;01min\u001b[39;00m completeness_analysis.values()),\n\u001b[32m    118\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdatasets_requiring_attention\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m([k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m completeness_analysis.items() \u001b[38;5;28;01mif\u001b[39;00m v[\u001b[33m'\u001b[39m\u001b[33mcleaning_recommendations\u001b[39m\u001b[33m'\u001b[39m]]),\n\u001b[32m    119\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcritical_imputation_needed\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m report \u001b[38;5;129;01min\u001b[39;00m completeness_analysis.values()\n\u001b[32m    120\u001b[39m                                         \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m report[\u001b[33m'\u001b[39m\u001b[33mcleaning_recommendations\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    121\u001b[39m                                         \u001b[38;5;28;01mif\u001b[39;00m rec.get(\u001b[33m'\u001b[39m\u001b[33maction\u001b[39m\u001b[33m'\u001b[39m) == \u001b[33m'\u001b[39m\u001b[33mCRITICAL_IMPUTATION\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    122\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlow_value_fields\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m report \u001b[38;5;129;01min\u001b[39;00m completeness_analysis.values()\n\u001b[32m    123\u001b[39m                               \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m report[\u001b[33m'\u001b[39m\u001b[33mcleaning_recommendations\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    124\u001b[39m                               \u001b[38;5;28;01mif\u001b[39;00m rec.get(\u001b[33m'\u001b[39m\u001b[33maction\u001b[39m\u001b[33m'\u001b[39m) == \u001b[33m'\u001b[39m\u001b[33mCONSIDER_DROPPING\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    125\u001b[39m     },\n\u001b[32m    126\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfile_manifest\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m    127\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbatch_directory\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(batch_dir),\n\u001b[32m    128\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m    129\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mquality_matrix\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(quality_file),\n\u001b[32m    130\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mcontamination_analysis\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(contamination_file),\n\u001b[32m    131\u001b[39m             \u001b[33m'\u001b[39m\u001b[33manomaly_detection\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(anomaly_file),\n\u001b[32m    132\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mbusiness_validation\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(business_file),\n\u001b[32m    133\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mcompleteness_analysis\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(completeness_file)\n\u001b[32m    134\u001b[39m         }\n\u001b[32m    135\u001b[39m     },\n\u001b[32m    136\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcleaning_priority_summary\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m    137\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcritical_actions_needed\u001b[39m\u001b[33m'\u001b[39m: (\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[43msummary\u001b[49m.get(\u001b[33m'\u001b[39m\u001b[33mbusiness_rules_summary\u001b[39m\u001b[33m'\u001b[39m, {}).get(\u001b[33m'\u001b[39m\u001b[33mtotal_violations\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m) +\n\u001b[32m    139\u001b[39m             summary.get(\u001b[33m'\u001b[39m\u001b[33mcompleteness_summary\u001b[39m\u001b[33m'\u001b[39m, {}).get(\u001b[33m'\u001b[39m\u001b[33mcritical_imputation_needed\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m    140\u001b[39m         ),\n\u001b[32m    141\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mhigh_priority_actions\u001b[39m\u001b[33m'\u001b[39m: summary.get(\u001b[33m'\u001b[39m\u001b[33mcontamination_summary\u001b[39m\u001b[33m'\u001b[39m, {}).get(\u001b[33m'\u001b[39m\u001b[33mtotal_contamination_types\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m),\n\u001b[32m    142\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_fields_analyzed\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(quality_matrix) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quality_matrix.empty \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    143\u001b[39m     }\n\u001b[32m    144\u001b[39m }\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Save comprehensive summary\u001b[39;00m\n\u001b[32m    147\u001b[39m summary_file = batch_dir / \u001b[33m\"\u001b[39m\u001b[33manalysis_summary.json\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'summary' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "# Fixed Save All Quality Analysis Results for Data Cleaning Workflow\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "def save_quality_analysis_results():\n",
        "    \"\"\"Save all data quality analysis results with organized batch folders.\"\"\"\n",
        "    print(\"SAVING QUALITY ANALYSIS RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create main results directory\n",
        "    results_dir = Path(\"../../data/quality_analysis\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create unique batch folder with descriptive naming\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    batch_name = f\"data_quality_batch_{timestamp}\"\n",
        "    batch_dir = results_dir / batch_name\n",
        "    batch_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    print(f\"   üìÅ Created batch folder: {batch_name}\")\n",
        "\n",
        "    # Generate comprehensive metadata\n",
        "    metadata = {\n",
        "        'batch_id': batch_name,\n",
        "        'analysis_timestamp': timestamp,\n",
        "        'analysis_datetime': datetime.now().isoformat(),\n",
        "        'datasets_analyzed': list(datasets.keys()),\n",
        "        'total_records': sum(len(df) for df in datasets.values()),\n",
        "        'analysis_components': [\n",
        "            'quality_matrix',\n",
        "            'contamination_analysis',\n",
        "            'anomaly_detection',\n",
        "            'business_validation',\n",
        "            'completeness_analysis'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # 1. Save Quality Matrix Results\n",
        "    quality_results = {\n",
        "        'quality_matrix': quality_matrix.to_dict('records') if not quality_matrix.empty else [],\n",
        "        'contamination_report': contamination_report,\n",
        "        'metadata': metadata\n",
        "    }\n",
        "\n",
        "    quality_file = batch_dir / \"quality_matrix.pkl\"\n",
        "    with open(quality_file, 'wb') as f:\n",
        "        pickle.dump(quality_results, f)\n",
        "    print(f\"   ‚úÖ Quality matrix: {quality_file.name}\")\n",
        "\n",
        "    # 2. Save Contamination Analysis\n",
        "    contamination_file = batch_dir / \"contamination_analysis.pkl\"\n",
        "    with open(contamination_file, 'wb') as f:\n",
        "        pickle.dump(contamination_analysis, f)\n",
        "    print(f\"   ‚úÖ Contamination analysis: {contamination_file.name}\")\n",
        "\n",
        "    # 3. Save Anomaly Detection Results\n",
        "    anomaly_file = batch_dir / \"anomaly_detection.pkl\"\n",
        "    with open(anomaly_file, 'wb') as f:\n",
        "        pickle.dump(anomalies, f)\n",
        "    print(f\"   ‚úÖ Anomaly detection: {anomaly_file.name}\")\n",
        "\n",
        "    # 4. Save Business Logic Validation\n",
        "    business_file = batch_dir / \"business_validation.pkl\"\n",
        "    with open(business_file, 'wb') as f:\n",
        "        pickle.dump(business_issues, f)\n",
        "    print(f\"   ‚úÖ Business validation: {business_file.name}\")\n",
        "\n",
        "    # 5. Save Completeness Analysis\n",
        "    completeness_file = batch_dir / \"completeness_analysis.pkl\"\n",
        "    with open(completeness_file, 'wb') as f:\n",
        "        pickle.dump(completeness_analysis, f)\n",
        "    print(f\"   ‚úÖ Completeness analysis: {completeness_file.name}\")\n",
        "\n",
        "    # 6. Calculate summary metrics first\n",
        "    # Quality metrics\n",
        "    overall_score = float(quality_matrix['quality_score'].mean()) if not quality_matrix.empty else 0\n",
        "    data_purity_score = float(quality_matrix['contamination_score'].mean()) if not quality_matrix.empty else 0\n",
        "    contaminated_fields = len(quality_matrix[quality_matrix['contamination_score'] < 0.95]) if not quality_matrix.empty else 0\n",
        "    critical_issues = len(quality_matrix[(quality_matrix['is_required']) & (quality_matrix['quality_score'] < 0.8)]) if not quality_matrix.empty else 0\n",
        "    excellent_fields = len(quality_matrix[quality_matrix['quality_score'] >= 0.95]) if not quality_matrix.empty else 0\n",
        "\n",
        "    # Contamination metrics\n",
        "    total_contamination_types = sum(\n",
        "        len(report.get('mixed_types', [])) +\n",
        "        len(report.get('special_chars', [])) +\n",
        "        len(report.get('encoding_issues', [])) +\n",
        "        len(report.get('format_inconsistencies', []))\n",
        "        for report in contamination_analysis.values()\n",
        "    )\n",
        "    datasets_with_contamination = len([k for k, v in contamination_analysis.items() if any([\n",
        "        v.get('mixed_types'), v.get('special_chars'),\n",
        "        v.get('encoding_issues'), v.get('format_inconsistencies')\n",
        "    ])])\n",
        "\n",
        "    # Business rules metrics\n",
        "    total_violations = sum(len(issues) for issues in business_issues.values())\n",
        "    datasets_with_violations = len([k for k, v in business_issues.items() if v])\n",
        "\n",
        "    # Completeness metrics\n",
        "    total_recommendations = sum(len(report['cleaning_recommendations']) for report in completeness_analysis.values())\n",
        "    total_consistency_issues = sum(len(report['consistency_issues']) for report in completeness_analysis.values())\n",
        "    critical_imputation_needed = sum(1 for report in completeness_analysis.values()\n",
        "                                    for rec in report['cleaning_recommendations']\n",
        "                                    if rec.get('action') == 'CRITICAL_IMPUTATION')\n",
        "\n",
        "    # 7. Create Comprehensive Summary (without circular references)\n",
        "    summary = {\n",
        "        'batch_info': metadata,\n",
        "        'quality_summary': {\n",
        "            'overall_score': overall_score,\n",
        "            'data_purity_score': data_purity_score,\n",
        "            'contaminated_fields': contaminated_fields,\n",
        "            'critical_issues': critical_issues,\n",
        "            'excellent_fields': excellent_fields\n",
        "        },\n",
        "        'contamination_summary': {\n",
        "            'total_contamination_types': total_contamination_types,\n",
        "            'datasets_with_contamination': datasets_with_contamination,\n",
        "            'mixed_type_issues': sum(len(report.get('mixed_types', [])) for report in contamination_analysis.values()),\n",
        "            'special_char_issues': sum(len(report.get('special_chars', [])) for report in contamination_analysis.values()),\n",
        "            'encoding_issues': sum(len(report.get('encoding_issues', [])) for report in contamination_analysis.values())\n",
        "        },\n",
        "        'anomaly_summary': {\n",
        "            'total_anomaly_types': sum(len(anomaly_list) for anomaly_list in anomalies.values()),\n",
        "            'high_severity_issues': sum(1 for anomaly_list in anomalies.values() for a in anomaly_list if a.get('severity') == 'HIGH'),\n",
        "            'medium_severity_issues': sum(1 for anomaly_list in anomalies.values() for a in anomaly_list if a.get('severity') == 'MEDIUM'),\n",
        "            'low_severity_issues': sum(1 for anomaly_list in anomalies.values() for a in anomaly_list if a.get('severity') == 'LOW'),\n",
        "            'datasets_with_anomalies': len([k for k, v in anomalies.items() if v])\n",
        "        },\n",
        "        'business_rules_summary': {\n",
        "            'total_violations': total_violations,\n",
        "            'datasets_with_violations': datasets_with_violations,\n",
        "            'violation_types': [issue for issues in business_issues.values() for issue in issues]\n",
        "        },\n",
        "        'completeness_summary': {\n",
        "            'total_recommendations': total_recommendations,\n",
        "            'total_consistency_issues': total_consistency_issues,\n",
        "            'datasets_requiring_attention': len([k for k, v in completeness_analysis.items() if v['cleaning_recommendations']]),\n",
        "            'critical_imputation_needed': critical_imputation_needed,\n",
        "            'low_value_fields': sum(1 for report in completeness_analysis.values()\n",
        "                                  for rec in report['cleaning_recommendations']\n",
        "                                  if rec.get('action') == 'CONSIDER_DROPPING')\n",
        "        },\n",
        "        'file_manifest': {\n",
        "            'batch_directory': str(batch_dir),\n",
        "            'files': {\n",
        "                'quality_matrix': str(quality_file),\n",
        "                'contamination_analysis': str(contamination_file),\n",
        "                'anomaly_detection': str(anomaly_file),\n",
        "                'business_validation': str(business_file),\n",
        "                'completeness_analysis': str(completeness_file)\n",
        "            }\n",
        "        },\n",
        "        'cleaning_priority_summary': {\n",
        "            'critical_actions_needed': total_violations + critical_imputation_needed,\n",
        "            'high_priority_actions': total_contamination_types,\n",
        "            'total_fields_analyzed': len(quality_matrix) if not quality_matrix.empty else 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save comprehensive summary\n",
        "    summary_file = batch_dir / \"analysis_summary.json\"\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(summary, f, indent=2, default=str)\n",
        "    print(f\"   ‚úÖ Analysis summary: {summary_file.name}\")\n",
        "\n",
        "    # 8. Create/Update Latest Directory (Always Points to Most Recent)\n",
        "    latest_dir = results_dir / \"latest\"\n",
        "\n",
        "    # Remove existing latest directory if it exists\n",
        "    if latest_dir.exists():\n",
        "        shutil.rmtree(latest_dir)\n",
        "\n",
        "    # Create fresh latest directory and copy all files\n",
        "    latest_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Copy all batch files to latest\n",
        "    for file_path in batch_dir.glob(\"*.pkl\"):\n",
        "        shutil.copy2(file_path, latest_dir / file_path.name)\n",
        "    for file_path in batch_dir.glob(\"*.json\"):\n",
        "        shutil.copy2(file_path, latest_dir / file_path.name)\n",
        "\n",
        "    # Create a latest_batch_info.txt for easy reference\n",
        "    batch_info_file = latest_dir / \"latest_batch_info.txt\"\n",
        "    with open(batch_info_file, 'w') as f:\n",
        "        f.write(f\"Latest Quality Analysis Batch\\n\")\n",
        "        f.write(f\"=\" * 30 + \"\\n\")\n",
        "        f.write(f\"Batch ID: {batch_name}\\n\")\n",
        "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Source Directory: {batch_dir}\\n\")\n",
        "        f.write(f\"Datasets Analyzed: {', '.join(list(datasets.keys()))}\\n\")\n",
        "        f.write(f\"Total Records: {sum(len(df) for df in datasets.values()):,}\\n\")\n",
        "        f.write(f\"\\nAnalysis Components:\\n\")\n",
        "        for component in metadata['analysis_components']:\n",
        "            f.write(f\"  - {component}.pkl\\n\")\n",
        "\n",
        "    print(f\"   ‚úÖ Latest directory updated: {latest_dir}\")\n",
        "\n",
        "    # 9. Create Batch Index for Easy Navigation\n",
        "    index_file = results_dir / \"batch_index.json\"\n",
        "\n",
        "    # Load existing index or create new\n",
        "    if index_file.exists():\n",
        "        with open(index_file, 'r') as f:\n",
        "            batch_index = json.load(f)\n",
        "    else:\n",
        "        batch_index = {'batches': []}\n",
        "\n",
        "    # Add current batch to index\n",
        "    batch_entry = {\n",
        "        'batch_id': batch_name,\n",
        "        'timestamp': timestamp,\n",
        "        'datetime': datetime.now().isoformat(),\n",
        "        'directory': str(batch_dir),\n",
        "        'datasets': list(datasets.keys()),\n",
        "        'total_records': sum(len(df) for df in datasets.values()),\n",
        "        'summary_scores': {\n",
        "            'overall_quality': overall_score,\n",
        "            'contaminated_fields': contaminated_fields,\n",
        "            'total_recommendations': total_recommendations\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Add to beginning of list (most recent first)\n",
        "    batch_index['batches'].insert(0, batch_entry)\n",
        "\n",
        "    # Keep only last 20 batches in index for performance\n",
        "    batch_index['batches'] = batch_index['batches'][:20]\n",
        "    batch_index['last_updated'] = datetime.now().isoformat()\n",
        "\n",
        "    with open(index_file, 'w') as f:\n",
        "        json.dump(batch_index, f, indent=2, default=str)\n",
        "\n",
        "    print(f\"   ‚úÖ Batch index updated: {index_file.name}\")\n",
        "\n",
        "    print(f\"\\nüéØ QUALITY ANALYSIS BATCH COMPLETE!\")\n",
        "    print(f\"   üìÇ Batch Directory: {batch_name}\")\n",
        "    print(f\"   üìä Overall Quality Score: {overall_score:.3f}\")\n",
        "    print(f\"   üîç Contaminated Fields: {contaminated_fields}\")\n",
        "    print(f\"   üîß Cleaning Recommendations: {total_recommendations}\")\n",
        "    print(f\"   üìã Business Rule Violations: {total_violations}\")\n",
        "\n",
        "    return summary, batch_name\n",
        "\n",
        "# Run the enhanced caching function\n",
        "analysis_summary, batch_id = save_quality_analysis_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
